{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6  \n",
    "# Data Encoding and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main focus of this chapter is using Python to process data presented in different kinds of common encodings, such as CSV files, JSON, XML, and binary packed records.  \n",
    "Unlike the chapter on data structures, this chapter is not focused on specific algorithms, but instead on the problem of getting data in and out of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Reading and Writing CSV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read or write data encoded as a CSV file, you can use Python's `csv` library.  \n",
    "We will use some stock market data from a CSV file for this example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stocks.csv\n",
    "\n",
    "Symbol,Price,Date,Time,Change,Volume\n",
    "\"AA\",39.48,\"6/11/2007\",\"9:36am\",-0.18,181800\n",
    "\"AIG\",71.38,\"6/11/2007\",\"9:36am\",-0.15,195500\n",
    "\"AXP\",62.58,\"6/11/2007\",\"9:36am\",-0.46,935000\n",
    "\"BA\",98.31,\"6/11/2007\",\"9:36am\",+0.12,104800\n",
    "\"C\",53.08,\"6/11/2007\",\"9:36am\",-0.25,360900\n",
    "\"CAT\",78.29,\"6/11/2007\",\"9:36am\",-0.23,225400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the data as a sequence of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # Process row\n",
    "        # ... and so forth\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, `row` will be a tuple.  \n",
    "Thus, to access certain fields, you will need to use indexing, such as `row[0]` (Symbol) and `row[4]` (Change).  \n",
    "Since such indexing can often be confusing, this is one place where you might want to consider the use of named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headings = next(f_csv)\n",
    "    Row = namedtuple('Row', headings)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        # Process row\n",
    "        # ... and so forth\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would allow you to use the column headers such as `row.Symbol` and `row.Change` instead of indices.  \n",
    "It should be noted that this only works if the column headers are valid Python identifiers.  \n",
    "If not, you might have to massage the initial headings (e.g., replacing nonidentifier characters with underscores or similar).  \n",
    "Another approach allows you to read the data as a sequence of dictionaries instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.DictReader(f)\n",
    "    for row in f_csv:\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, youo would access the elements of each row using the row headers.  \n",
    "For example, `row['Symbol']` or `row['Change']`.  \n",
    "To write CSV data, you also use the `csv` module, but you create a writer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol','Price','Date','Time','Change','Volume']\n",
    "rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800),\n",
    "            ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500),\n",
    "            ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv', 'w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the data as a sequence of dictionaries, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
    "rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.18, 'Volume':181800},\n",
    "        {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.15, 'Volume': 195500},\n",
    "        {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.46, 'Volume': 935000},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv', 'w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `csv` module can save you quite a bit of time over parsing, splitting, and cleaning the data manually by yourself.  \n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv') as f:\n",
    "    for line in f:\n",
    "        row = line.split(',')\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that you’ll still need to deal with some nasty details.  \n",
    "For example, if any of the fields are surrounded by quotes, you’ll have to strip the quotes.  \n",
    "In addition, if a quoted field happens to contain a comma, the code will break by producing a row with the wrong size.  \n",
    "By default, the `csv` library is programmed to understand CSV encoding rules used by Microsoft Excel.  \n",
    "This is probably the most common variant, and will likely give you the best compatibility.  \n",
    "However, if you consult the documentation for csv, you’ll see a few ways to tweak the encoding to different formats (e.g., changing the separator character, etc.).  \n",
    "For example, if you want to read tab-delimited data instead, use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv') as f:\n",
    "    f_tsv = csv.reader(f, delimiter='\\t')\n",
    "    for row in f_tsv:\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're reading CSV data and converting it into named tuples, use caution when validating column headers.  \n",
    "For example, a CSV file could have a header line containing nonvalid identifier characters like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Street Address,Num-Premises,Latitude,Longitude`  \n",
    "`5412 N CLARK,10,41.980262,-87.668452`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will actually cause the creation of a `namedtuple` to fail with a `ValueError` exception.  \n",
    "To work around this, you might have to scrub the headers first.  \n",
    "For instance, carrying a regex substitution on nonvalid identifier characters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]\n",
    "    Row = namedtuple('Row', headers)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        # do something\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that `csv` does not try to interpret the data or convert it to a type other than a string.  \n",
    "The following example performs extra type conversions on CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = [str, float, str, str, float, int]\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # Apply conversions to the row items\n",
    "        row = tuple(convert(value) for convert, value in zip(col_types, row))\n",
    "        # And so forth ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert selected fields of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading as dicts with type conversion\n",
      "OrderedDict([('Symbol', 'AA'), ('Price', 39.48), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.18), ('Volume', 181800)])\n",
      "OrderedDict([('Symbol', 'AIG'), ('Price', 71.38), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.15), ('Volume', 195500)])\n",
      "OrderedDict([('Symbol', 'AXP'), ('Price', 62.58), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.46), ('Volume', 935000)])\n"
     ]
    }
   ],
   "source": [
    "print('Reading as dicts with type conversion')\n",
    "field_types = [ ('Price', float),\n",
    "                ('Change', float),\n",
    "                ('Volume', int) ]\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row.update((key, conversion(row[key])) for key, conversion in field_types)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you’ll probably want to be a bit careful with such conversions, though.  \n",
    "In the real world, it’s common for CSV files to have missing values, corrupted data, and other issues that would break type conversions.  \n",
    "So, unless your data is guaranteed to be error free, that’s something you’ll need to consider (you might need to add suitable exception handling).  \n",
    "Finally, if your goal in reading CSV data is to perform data analysis and statistics, you might want to look at the `pandas` package.  \n",
    "`pandas` includes a convenient `pandas.read_csv()` function that will load CSV data into a `DataFrame` object.  \n",
    "From there, you can generate various summary statistics, filter the data, and perform other kinds of high-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Reading and Writing JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem  \n",
    "You want to read or write data encoded as JavaScript Object Notation (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution  \n",
    "The `json` module provides an easy way to encode and decode data in JSON.  \n",
    "The two main functions are `json.dumps()` and `json.loads()`, mirroring the interface used in other serialization libraries, such as `pickle`.  \n",
    "Here is how you turn a Python data structure into JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"ACME\", \"shares\": 100, \"price\": 542.23}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    'name' : 'ACME',\n",
    "    'shares' : 100,\n",
    "    'price': 542.23\n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn the JSON-encoded string back into a Python data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ACME', 'shares': 100, 'price': 542.23}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(json_str); data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with files instead of strings, you can also use `json.dump()` and `json.load()` to encode and decode JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ACME', 'shares': 100, 'price': 542.23}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the data\n",
    "with open ('data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "    \n",
    "# Read data back\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion  \n",
    "JSON encoding supports the basic types of `None, bool, int, float,` and `str`, as well as lists, tuples, and dictionaries containing those types.  \n",
    "For dictionaries, keys are assumed to be strings (any non-string keys in a dictionary are converted to strings during encoding).  \n",
    "To be compliant with the JSON specification, you should only encode Python lists and dictionaries.  \n",
    "Note that in web applications, it is also conventional for the top-level object to be a dictionary.  \n",
    "The format of JSON encoding is almost identical to Python syntax except for a few minor changes.  \n",
    "for instance, `True` is mapped to `true`, `False` is mapped to `false`, and `None` is mapped to `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"a\": true, \"b\": \"Hello\", \"c\": null}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'a' : True,\n",
    "    'b' : 'Hello',\n",
    "    'c': None\n",
    "}\n",
    "\n",
    "json.dumps(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are trying to examine data you have decoded from JSON, it can often be hard to ascertain its structure simply by printing it out, especially if the data contains a deep level of nested structures or a lot of fields.  \n",
    "To assist with this, consider using the `pprint()` function in the pprint module.  \n",
    "This will alphabetize the keys and output a dictionary in a more sane way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, JSON decoding will create dicts or lists from the supplied data.  \n",
    "If you want to create different kinds of objects, supply the `object_pairs_hook` or `object_hook` to `json.loads()`.  \n",
    "Here is one way you can encode JSON data that preserves its order in an `OrderedDict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "data = json.loads(s, object_pairs_hook=OrderedDict); data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also turn a JSON dictionary into a Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ACME', 50, 490.1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JSONObject:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "        \n",
    "data = json.loads(s, object_hook=JSONObject)\n",
    "data.name, data.shares, data.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, the dictionary created by decoding the JSON data is passed as a single argument to `__init__()`.  \n",
    "From there, you can use it directly as the instance dictionary of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few options that can be useful for encoding JSON.  \n",
    "If you would like the output to be nicely formatted, you can use the indent argument to `json.dumps()`.  \n",
    "This causes the output to be pretty printed in a format similar to that with the `pprint()` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"ACME\", \"shares\": 100, \"price\": 542.23}\n",
      "{\n",
      "    \"name\": \"ACME\",\n",
      "    \"shares\": 100,\n",
      "    \"price\": 542.23\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(json.dumps(data))\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `sort_keys` argument to sort the keys alphabetically on output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"ACME\", \"price\": 542.23, \"shares\": 100}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances are not normally serializable as JSON.  \n",
    "The following code breaks down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "p = Point(2, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json.dumps(p)\n",
    "\n",
    ">>>>>>>\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-25-e93da69829e6> in <module>()\n",
    "      5 \n",
    "      6 p = Point(2, 3)\n",
    "----> 7 json.dumps(p)\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n",
    "    229         cls is None and indent is None and separators is None and\n",
    "    230         default is None and not sort_keys and not kw):\n",
    "--> 231         return _default_encoder.encode(obj)\n",
    "    232     if cls is None:\n",
    "    233         cls = JSONEncoder\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in encode(self, o)\n",
    "    197         # exceptions aren't as detailed.  The list call should be roughly\n",
    "    198         # equivalent to the PySequence_Fast that ''.join() would do.\n",
    "--> 199         chunks = self.iterencode(o, _one_shot=True)\n",
    "    200         if not isinstance(chunks, (list, tuple)):\n",
    "    201             chunks = list(chunks)\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)\n",
    "    255                 self.key_separator, self.item_separator, self.sort_keys,\n",
    "    256                 self.skipkeys, _one_shot)\n",
    "--> 257         return _iterencode(o, 0)\n",
    "    258 \n",
    "    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in default(self, o)\n",
    "    178         \"\"\"\n",
    "    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n",
    "--> 180                         o.__class__.__name__)\n",
    "    181 \n",
    "    182     def encode(self, o):\n",
    "\n",
    "TypeError: Object of type 'Point' is not JSON serializable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to serialize instances, you can supply a function that takes an instance as input and returns a dictionary that can be serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_instance(obj):\n",
    "    d = { '__classname__' : type(obj).__name__ }\n",
    "    d.update(vars(obj))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an instance back, you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping names to known classes\n",
    "classes = { 'Point' : Point }\n",
    "\n",
    "def unserialize_object(d):\n",
    "    clsname = d.pop('__classname__', None)\n",
    "    if clsname:\n",
    "        cls = classes[clsname]\n",
    "        obj = cls.__new__(cls)  # Creates an instance without calling the __init__() method\n",
    "        for key, value in d.items():\n",
    "            setattr(obj, key, value)\n",
    "            return obj\n",
    "    else:\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"__classname__\": \"Point\", \"x\": 2, \"y\": 3}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Point(2,3)\n",
    "s = json.dumps(p, default=serialize_instance); s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Point at 0x10825e748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = json.loads(s, object_hook=unserialize_object); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `json` module has a variety of other options for controlling the low-level interpretation of numbers, special values such as `NaN`, and more.  \n",
    "[The JavaScript Object Notation (JSON) Data Interchange Format](https://tools.ietf.org/html/rfc8259)  \n",
    "[`json` — JSON encoder and decoder](https://docs.python.org/3.7/library/json.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Parsing Simple XML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xml.etree.ElementTree` module can be used to extract data from simple XML documents.  \n",
    "To illustrate, suppose you want to parse and make a summary of the RSS feed on [Planet Python](https://planetpython.org/).  \n",
    "The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x1082bb198>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "# Download the RSS feed and parse it:\n",
    "u = urlopen('https://planet.python.org/rss20.xml')\n",
    "doc = parse(u); doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract and output the tags that interest us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tryton News: Newsletter March 2019\n",
      "Fri, 01 Mar 2019 07:00:00 +0000\n",
      "https://discuss.tryton.org/t/newsletter-march-2019/1104\n",
      "\n",
      "Toshio Kuratomi: Managing vim8 plugins\n",
      "Fri, 01 Mar 2019 02:02:50 +0000\n",
      "https://anonbadger.wordpress.com/2019/02/28/managing-vim8-plugins/\n",
      "\n",
      "Test and Code: 67: Teaching Python in Middle School\n",
      "Thu, 28 Feb 2019 18:00:00 +0000\n",
      "https://testandcode.com/67\n",
      "\n",
      "Stack Abuse: Doubly Linked List with Python Examples\n",
      "Thu, 28 Feb 2019 17:24:00 +0000\n",
      "https://stackabuse.com/doubly-linked-list-with-python-examples/\n",
      "\n",
      "PyCharm: PyCharm 2019.1 EAP 6\n",
      "Thu, 28 Feb 2019 09:47:33 +0000\n",
      "http://feedproxy.google.com/~r/Pycharm/~3/23NYxhDg7V4/\n",
      "\n",
      "gamingdirectional: The mana detection mechanism\n",
      "Thu, 28 Feb 2019 07:04:26 +0000\n",
      "http://gamingdirectional.com/blog/2019/02/28/the-mana-detection-mechanism/\n",
      "\n",
      "codingdirectional: Include the currency name into the forex application\n",
      "Thu, 28 Feb 2019 04:30:39 +0000\n",
      "http://codingdirectional.info/2019/02/28/include-the-currency-name-into-the-forex-application/\n",
      "\n",
      "PyCharm: PyCharm 2018.3.5\n",
      "Wed, 27 Feb 2019 14:32:19 +0000\n",
      "http://feedproxy.google.com/~r/Pycharm/~3/s4LxQEv6eq0/\n",
      "\n",
      "Stack Abuse: Introduction to the Python Pathlib Module\n",
      "Wed, 27 Feb 2019 14:01:00 +0000\n",
      "https://stackabuse.com/introduction-to-the-python-pathlib-module/\n",
      "\n",
      "Real Python: Traditional Face Detection With Python\n",
      "Wed, 27 Feb 2019 14:00:00 +0000\n",
      "https://realpython.com/traditional-face-detection-python/\n",
      "\n",
      "Talk Python to Me: #201 Choosing JupyterHub and Python over MATLAB\n",
      "Wed, 27 Feb 2019 08:00:00 +0000\n",
      "https://talkpython.fm/episodes/show/201/choosing-jupyterhub-and-python-over-matlab\n",
      "\n",
      "Mike Driscoll: Pros and Cons of Indy Publishing\n",
      "Wed, 27 Feb 2019 06:05:42 +0000\n",
      "http://www.blog.pythonlibrary.org/2019/02/27/pros-and-cons-of-indy-publishing/\n",
      "\n",
      "Reuven Lerner: Want to improve your Python skills?  Join the upcoming cohort of Weekly Python Exercise!\n",
      "Tue, 26 Feb 2019 21:51:28 +0000\n",
      "https://blog.lerner.co.il/want-to-improve-your-python-skills-join-the-upcoming-cohort-of-weekly-python-exercise/\n",
      "\n",
      "Python Engineering at Microsoft: Python in Visual Studio Code  – February 2019 Release\n",
      "Tue, 26 Feb 2019 21:37:03 +0000\n",
      "https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2019-release/\n",
      "\n",
      "Test and Code: 66: Brian is interviewed by Phil Burgess\n",
      "Tue, 26 Feb 2019 21:00:00 +0000\n",
      "https://testandcode.com/66\n",
      "\n",
      "PyCoder’s Weekly: Issue #357 (Feb. 26, 2019)\n",
      "Tue, 26 Feb 2019 20:30:00 +0000\n",
      "https://pycoders.com/issues/357\n",
      "\n",
      "NumFOCUS: Blosc joins NumFOCUS Sponsored Projects\n",
      "Tue, 26 Feb 2019 19:24:14 +0000\n",
      "https://numfocus.org/blog/blosc-joins-numfocus-sponsored-projects\n",
      "\n",
      "Artem Rys: RabbitMQ Scrapy Item Publisher in Python\n",
      "Tue, 26 Feb 2019 15:33:12 +0000\n",
      "https://medium.com/python4you/rabbitmq-scrapy-item-publisher-in-python-4c66a985e3cb?source=rss----5527f69f4771---4\n",
      "\n",
      "Mike Driscoll: How I Write Books about Python\n",
      "Tue, 26 Feb 2019 13:30:47 +0000\n",
      "http://www.blog.pythonlibrary.org/2019/02/26/how-i-write-books-about-python/\n",
      "\n",
      "codingdirectional: Continue developing the currency exchange application\n",
      "Tue, 26 Feb 2019 12:33:21 +0000\n",
      "http://codingdirectional.info/2019/02/26/continue-developing-the-currency-exchange-application/\n",
      "\n",
      "Ian Ozsvald: On the Delivery of Data Science Projects &#8211; talk for Business, Analytics and Data Science meetup\n",
      "Tue, 26 Feb 2019 11:02:06 +0000\n",
      "https://ianozsvald.com/2019/02/26/on-the-delivery-of-data-science-projects-talk-for-business-analytics-and-data-science-meetup/\n",
      "\n",
      "PyBites: Generating Beautiful Code Snippets with Carbon and Selenium\n",
      "Tue, 26 Feb 2019 11:00:00 +0000\n",
      "https://pybit.es/python-tips-carbon-selenium.html\n",
      "\n",
      "Python Bytes: #119 Assorted files as Django ORM backends with Alkali\n",
      "Tue, 26 Feb 2019 08:00:00 +0000\n",
      "https://pythonbytes.fm/episodes/show/119/assorted-files-as-django-orm-backends-with-alkali\n",
      "\n",
      "gamingdirectional: Create the mana object with Pygame\n",
      "Tue, 26 Feb 2019 06:23:29 +0000\n",
      "http://gamingdirectional.com/blog/2019/02/26/create-the-mana-object-with-pygame/\n",
      "\n",
      "Yasoob Khalid: Python dis module and constant folding\n",
      "Tue, 26 Feb 2019 05:48:21 +0000\n",
      "https://pythontips.com/2019/02/26/python-dis-module-and-constant-folding/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in doc.iterfind('channel/item'):\n",
    "    title = item.findtext('title')\n",
    "    date = item.findtext('pubDate')\n",
    "    link = item.findtext('link')\n",
    "    print(title)\n",
    "    print(date)\n",
    "    print(link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data encoded as XML is commonplace in many applications.  \n",
    "Not only is XML widely used as a format for exchanging data on the Internet, it is a common format for storing application data (e.g., word processing, music libraries, etc.).  \n",
    "The discussion that follows already assumes the reader is familiar with XML basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, when XML is simply being used to store data, the document structure is compact and straightforward.  \n",
    "The `xml.etree.ElementTree.parse()` function parses the entire XML document into a document object.  \n",
    "From there, you use methods such as `find()`, `iterfind()`, and `findtext()` to search for specific XML elements.  \n",
    "The arguments to these functions are the names of a specific tag, such as channel/item or title.\n",
    "When specifying tags, you need to take the overall document structure into account.  \n",
    "Each find operation takes place relative to a starting element. \n",
    "Likewise, the tagname that you supply to each operation is also relative to the start.  \n",
    "In the example, the call to `doc.iterfind('channel/item')` looks for all \"item\" elements under a \"channel\" element. doc represents the top of the document (the top-level \"rss\" element).  \n",
    "The later calls to `item.findtext()` take place relative to the found \"item\" elements.  \n",
    "Each element represented by the `ElementTree` module has a few essential attributes and methods that are useful when parsing.  \n",
    "The tag attribute contains the name of the tag, the text attribute contains enclosed text, and the `get()` method can be used to extract attributes (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x1082bb198>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'title' at 0x1082eaf98>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = doc.find('channel/title'); e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planet Python'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that `xml.etree.ElementTree` is not the only option for XML parsing.  \n",
    "For more advanced applications, you might consider `lxml`.  \n",
    "It uses the same program‐ ming interface as ElementTree, so the example shown in this recipe works in the same manner.  \n",
    "You simply need to change the first import to:  \n",
    "`from lxml.etree import parse`.  \n",
    "`lxml` provides the benefit of being fully compliant with XML standards.  \n",
    "It is also extremely fast, and provides support for features such as validation, XSLT, and XPath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Parsing Huge XML Files Incrementally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to extract data from a huge XML document while using as little memory as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you are faced with the problem of incremental data processing, you should think of iterators and generators.  \n",
    "Here is a simple function that can be used to incrementally process huge XML files using a very small memory footprint:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "\n",
    "def parse_and_remove(filename, path):\n",
    "    path_parts = path.split('/')\n",
    "    doc = iterparse(filename, ('start', 'end'))\n",
    "    # Skip the root element:\n",
    "    next(doc)\n",
    "    \n",
    "    tag_stack = []\n",
    "    elem_stack = []\n",
    "    for event, elem in doc:\n",
    "        if event == 'start':\n",
    "            tag_stack.append(elem.tag)\n",
    "            elem_stack.append(elem)\n",
    "        elif even == 'end':\n",
    "            if tag_stack == path_parts:\n",
    "                yield elem\n",
    "                elem_stack[-2].remove(elem)\n",
    "            try:\n",
    "                tag_stack.pop()\n",
    "                elem_stack.pop()\n",
    "            except IndexError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function, you now need to find a large XML file to work with.  \n",
    "You can often find such files on government and open data websites.  \n",
    "For example, you can download [Chicago’s pothole database](https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Pot-Holes-Reported/7as2-ds3y) as XML.  \n",
    "At the time of this writing, the downloaded file consists of more than 100,000 rows of data, which are encoded like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<response>\n",
    "    <row>\n",
    "       <row ...>\n",
    "          <creation_date>2012-11-18T00:00:00</creation_date>\n",
    "            <status>Completed</status>\n",
    "            <completion_date>2012-11-18T00:00:00</completion_date>\n",
    "            <service_request_number>12-01906549</service_request_number>\n",
    "            <type_of_service_request>Pot Hole in Street</type_of_service_request>\n",
    "            <current_activity>Final Outcome</current_activity>\n",
    "            <most_recent_action>CDOT Street Cut ... Outcome</most_recent_action>\n",
    "            <street_address>4714 S TALMAN AVE</street_address>\n",
    "            <zip>60632</zip>\n",
    "            <x_coordinate>1159494.68618856</x_coordinate>\n",
    "            <y_coordinate>1873313.83503384</y_coordinate>\n",
    "            <ward>14</ward>\n",
    "            <police_district>9</police_district>\n",
    "            <community_area>58</community_area>\n",
    "            <latitude>41.808090232127896</latitude>\n",
    "            <longitude>-87.69053684711305</longitude>\n",
    "            <location latitude=\"41.808090232127896\"\n",
    "        </row>\n",
    "        <row ...>\n",
    "        longitude=\"-87.69053684711305\" />\n",
    "        <creation_date>2012-11-18T00:00:00</creation_date>\n",
    "        <status>Completed</status>\n",
    "        <completion_date>2012-11-18T00:00:00</completion_date>\n",
    "        <service_request_number>12-01906695</service_request_number>\n",
    "        <type_of_service_request>Pot Hole in Street</type_of_service_request>\n",
    "        <current_activity>Final Outcome</current_activity>\n",
    "        <most_recent_action>CDOT Street Cut ... Outcome</most_recent_action>\n",
    "        <street_address>3510 W NORTH AVE</street_address>\n",
    "        <zip>60647</zip>\n",
    "        <x_coordinate>1152732.14127696</x_coordinate>\n",
    "        <y_coordinate>1910409.38979075</y_coordinate>\n",
    "        <ward>26</ward>\n",
    "        <police_district>14</police_district>\n",
    "        <community_area>23</community_area>\n",
    "        <latitude>41.91002084292946</latitude>\n",
    "        <longitude>-87.71435952353961</longitude>\n",
    "        <location latitude=\"41.91002084292946\"\n",
    "        longitude=\"-87.71435952353961\" />\n",
    "      </row>\n",
    "   </row>\n",
    "</response>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could write a script that ranks ZIP codes by the number of pothole reports:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from xml.etree.ElementTree import parse\n",
    "from collections import Counter\n",
    "\n",
    "potholes_by_zip = Counter()\n",
    "doc = parse(potholes.xml)\n",
    "for pothole in doc.iterfind('row/row'):\n",
    "    potholes_by_zip[pothole.findtext('zip')] += 1\n",
    "for zipcode, num in potholes_by_zip.most_common():\n",
    "    print(zipcode, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only problem with this script is that it reads and parses the entire XML file into memory.  \n",
    "On our machine, it takes about 450 MB of memory to run.  \n",
    "Using this recipe’s code, the program changes only slightly:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "potholes_by_zip = Counter()\n",
    "data = parse_and_remove('potholes.xml', 'row/row')\n",
    "for pothole in data:\n",
    "    potholes_by_zip[pothole.findtext('zip')] += 1\n",
    "for zipcode, num in potholes_by_zip.most_common():\n",
    "    print(zipcode, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the program has a memory footprint of only 7MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe relies on two core features of the `ElementTree` module.  \n",
    "First, the `iterparse()` method allows incremental processing of XML documents.  \n",
    "To use it, you supply the filename along with an event list consisting of one or more of the following:  \n",
    "`start, end, start-ns,` and `end-ns`.  \n",
    "The iterator created by `iterparse()` produces tuples of the form `(event, elem)`, where `event` is one of the listed events and `elem` is the resulting XML element."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> data = iterparse('potholes.xml',('start','end'))\n",
    ">>> next(data)\n",
    "('start', <Element 'response' at 0x100771d60>)\n",
    ">>> next(data)\n",
    "('start', <Element 'row' at 0x100771e68>)\n",
    ">>> next(data)\n",
    "('start', <Element 'row' at 0x100771fc8>)\n",
    ">>> next(data)\n",
    "('start', <Element 'creation_date' at 0x100771f18>)\n",
    ">>> next(data)\n",
    "('end', <Element 'creation_date' at 0x100771f18>)\n",
    ">>> next(data)\n",
    "('start', <Element 'status' at 0x1006a7f18>)\n",
    ">>> next(data)\n",
    "('end', <Element 'status' at 0x1006a7f18>)\n",
    ">>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`start` events are created when an element is first created but not yet populated with any other data (e.g., child elements).  \n",
    "`end` events are created when an element is completed.  \n",
    "Although not shown in this recipe, `start-ns` and `end-ns` events are used to handle XML namespace declarations.  \n",
    "In this recipe, the start and end events are used to manage stacks of elements and tags.  \n",
    "The stacks represent the current hierarchical structure of the document as it’s being parsed, and are also used to determine if an element matches the requested path given to the `parse_and_remove()` function.  \n",
    "If a match is made, `yield` is used to emit it back to the caller.  \n",
    "The following statement after the yield is the core feature of ElementTree that makes this recipe save memory:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`elem_stack[-2].remove(elem)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement causes the previously yielded element to be removed from its parent.  \n",
    "Assuming that no references are left to it anywhere else, the element is destroyed and memory reclaimed.  \n",
    "The end effect of the iterative parse and the removal of nodes is a highly efficient incremental sweep over the document.  \n",
    "At no point is a complete document tree ever constructed.  \n",
    "Yet, it is still possible to write code that processes the XML data in a straightforward manner.  \n",
    "The primary downside to this recipe is its runtime performance.  \n",
    "When tested, the version of code that reads the entire document into memory first runs approximately twice as fast as the version that processes it incrementally.  \n",
    "However, it requires more than 60 times as much memory.  \n",
    "So, if memory use is a greater concern, the incremental version is a big win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Turning A Dictionary into XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data in a Python dictionary and convert it to XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the `xml.etree.ElementTree` library is commonly used for parsing, it can also be used to create XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'stock' at 0x109430b88>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.etree.ElementTree import Element\n",
    "\n",
    "def dict_to_xml(tag, d):\n",
    "    \"\"\"\n",
    "    Turn a dict into XML\n",
    "    \"\"\"\n",
    "    elem = Element(tag)\n",
    "    for key, val in d.items():\n",
    "        child = Element(key)\n",
    "        child.text = str(val)\n",
    "        elem.append(child)\n",
    "    return elem\n",
    "\n",
    "s = { 'name': 'GOOG', 'shares': 100, 'price':490.1 }\n",
    "e = dict_to_xml('stock', s)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this conversion is an `Element` instance.  \n",
    "For I/O, it's easy to convert this instance to a byte string using the `tostring()` function in `xml.etree.ElementTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<stock><name>GOOG</name><shares>100</shares><price>490.1</price></stock>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.etree.ElementTree import tostring \n",
    "\n",
    "tostring(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also attach attributes to an element using its `set()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<stock _id=\"1234\"><name>GOOG</name><shares>100</shares><price>490.1</price></stock>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.set('_id', '1234')\n",
    "tostring(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the order of the elements matters, you might make an `OrderedDict` instead of a normal dictionary, like in Recipe 1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating XML, you might be inclined to just make strings instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_xml_str(tag, d):\n",
    "    \"\"\"\n",
    "    Turn a simple dict of key/value pairs into XML\n",
    "    \"\"\"\n",
    "    parts = ['<{}>'.format(tag)]\n",
    "    for key, val in d.items():\n",
    "        parts.append('<{0}>{1}</{0}>'.format(key, val))\n",
    "    parts.append('</{}>'.format(tag))\n",
    "    return ''.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you try to do things manually, things can become messy.  \n",
    "How do you deal with special characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<item><name><spam></name></item>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = { 'name' : '<spam>'}\n",
    "# String creation:\n",
    "dict_to_xml_str('item', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<item><name>&lt;spam&gt;</name></item>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proper XML creation:\n",
    "e = dict_to_xml('item', d)\n",
    "tostring(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the latter example, the characters `<` and `>` got replaced with `&lt;` and `&gt;`.  \n",
    "Just for reference, if you ever need to manually escape or unescape such characters, you can use the `escape()` and `unescape()` functions in `xml.sax.saxutils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&lt;spam&gt;'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.sax.saxutils import escape, unescape\n",
    "\n",
    "escape('<spam>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<spam>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unescape(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from creating correct output, the other reason why it’s a good idea to create `Element` instances instead of strings is that they can be more easily combined together to make a larger document.  \n",
    "The resulting `Element` instances can also be processed in various ways without ever having to worry about parsing the XML text.  \n",
    "Essentially, you can do all of the processing of the data in a more high-level form and then output it as a string at the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Parsing, Modifying, and Rewriting XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to read an XML document, make changes to it, and then write it back out as XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xml.etree.ElementTree` module makes it easy to perform such tasks.  \n",
    "Essentially, you start out by parsing the document in the usual way.  \n",
    "For example, suppose you have a document named `pred.xml` that looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\"?>\n",
    "<stop>\n",
    "    <id>14791</id>\n",
    "    <nm>Clark &amp; Balmoral</nm>\n",
    "    <sri>\n",
    "        <rt>22</rt>\n",
    "        <d>North Bound</d>\n",
    "        <dd>North Bound</dd>\n",
    "    </sri>\n",
    "    <cr>22</cr>\n",
    "    <pre>\n",
    "        <pt>5 MIN</pt>\n",
    "        <fd>Howard</fd>\n",
    "        <v>1378</v>\n",
    "        <rn>22</rn>\n",
    "    </pre>\n",
    "    <pre>\n",
    "        <pt>15 MIN</pt>\n",
    "        <fd>Howard</fd>\n",
    "        <v>1867</v>\n",
    "        <rn>22</rn>\n",
    "    </pre>\n",
    "</stop>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `ElementTree` to read it and make changes to the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'stop' at 0x10945c958>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.etree.ElementTree import parse, Element\n",
    "\n",
    "doc = parse('pred.xml')\n",
    "root = doc.getroot()\n",
    "root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some changes to our XML file and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove a few elements\n",
    "root.remove(root.find('sri'))\n",
    "root.remove(root.find('cr'))\n",
    "# Insert a new element after <nm>...</nm>\n",
    "root.getchildren().index(root.find('nm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a simple element that will be added to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Element('spam')\n",
    "e.text = 'This is a test'\n",
    "root.insert(2, e)\n",
    "# Write it to the file:\n",
    "doc.write('newpred.xml', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a new XML file that looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version='1.0' encoding='us-ascii'?>\n",
    "<stop>\n",
    "    <id>14791</id>\n",
    "    <nm>Clark &amp; Balmoral</nm>\n",
    "    <spam>This is a test</spam><pre>\n",
    "        <pt>5 MIN</pt>\n",
    "        <fd>Howard</fd>\n",
    "        <v>1378</v>\n",
    "        <rn>22</rn>\n",
    "    </pre>\n",
    "    <pre>\n",
    "        <pt>15 MIN</pt>\n",
    "        <fd>Howard</fd>\n",
    "        <v>1867</v>\n",
    "        <rn>22</rn>\n",
    "    </pre>\n",
    "</stop>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the structure of an XML document is straightforward, but you must remember that all modifications are generally made to the parent element, treating it as if it were a list.  \n",
    "For example, if you remove an element, it is removed from its immediate parent using that parent’s `remove()` method.  \n",
    "If you insert or append new elements, you also use `insert()` and `append()` methods on the parent.  \n",
    "Elements can also be manipulated using indexing and slicing operations, such as `element[i]` or `element[i:j]`.  \n",
    "If you need to make new elements, use the `Element` class, as shown in this recipe’s solution.  \n",
    "A further description is available in Recipe 6.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Parsing XML Documents with Namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to parse an XML document, but it uses XML namespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how the following document uses namespaces:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<top>\n",
    "    <author>David Beazley</author>\n",
    "    <content>\n",
    "        <html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
    "            <head>\n",
    "                <title>Hello World</title>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>Hello World!</h1>\n",
    "            </body>\n",
    "        </html>\n",
    "    </content>\n",
    "</top>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you parse this document and try to perform the usual queries, you'll find that it doesn't work so easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse('namespaces.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with some queries that actually work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David Beazley'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.findtext('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'content' at 0x109467638>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try some queries that don't go so well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A query involving a namespace:\n",
    "doc.find('content/html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a fully qualified query will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element '{http://www.w3.org/1999/xhtml}html' at 0x109467688>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('content/{http://www.w3.org/1999/xhtml}html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one doesn't work either:\n",
    "doc.findtext('content/{http://www.w3.org/1999/xhtml}html/head/title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully qualified:\n",
    "doc.findtext('content/{http://www.w3.org/1999/xhtml}html/'\\\n",
    "             '{http://www.w3.org/1999/xhtml}head/{http://www.w3.org/1999/xhtml}title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way that you can simplify things is to wrap namespace handling up into a utility class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XMLNamespaces:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.namespaces = {}\n",
    "        for name, uri in kwargs.items():\n",
    "            self.register(name, uri)\n",
    "    def register(self, name, uri):\n",
    "        self.namespaces[name] = '{'+uri+'}'\n",
    "    def __call__(self, path):\n",
    "        return path.format_map(self.namespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put our class to work making our lives easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element '{http://www.w3.org/1999/xhtml}html' at 0x109467688>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = XMLNamespaces(html='http://www.w3.org/1999/xhtml')\n",
    "doc.find(ns('content/{html}html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.findtext(ns('content/{html}html/{html}head/{html}title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing XML documents that contain namespaces can be messy.  \n",
    "The `XMLNamespaces` class is really just meant to clean it up slightly by allowing you to use the shortened namespace names in subsequent operations as opposed to fully qualified URIs.  \n",
    "Unfortunately, there is no mechanism in the basic `ElementTree` parser to get further information about namespaces.  \n",
    "However, you can get a bit more information about the scope of namespace processing if you’re willing to use the `iterparse()` function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end <Element 'author' at 0x10946f9a8>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}title' at 0x10946fae8>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}head' at 0x10946fa98>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}h1' at 0x10946fb88>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}body' at 0x10946fb38>\n",
      "end <Element '{http://www.w3.org/1999/xhtml}html' at 0x10946fa48>\n",
      "end <Element 'content' at 0x10946f9f8>\n",
      "end <Element 'top' at 0x10946f958>\n"
     ]
    }
   ],
   "source": [
    "from xml.etree.ElementTree import iterparse\n",
    "\n",
    "for evt, elem in iterparse('namespaces.xml'):\n",
    "    print(evt, elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'top' at 0x10946f958>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The top-most element:\n",
    "elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, if the text you are parsing makes use of namespaces in addition to other advanced XML features, you’re really better off using the `lxml` library instead of `ElementTree`.  \n",
    "For instance, `lxml` provides better support for validating documents against a DTD, more complete XPath support, and other advanced XML features.  \n",
    "This recipe is really just a simple fix to make parsing a little easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Interacting with a Relational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to select, insert, or delete rows in a relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of representing rows of data in Python is as a sequence of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GOOG', 100, 490.1),\n",
       " ('AAPL', 50, 545.75),\n",
       " ('FB', 150, 7.45),\n",
       " ('HPQ', 75, 33.2)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks = [\n",
    "        ('GOOG', 100, 490.1),\n",
    "        ('AAPL', 50, 545.75),\n",
    "        ('FB', 150, 7.45),\n",
    "        ('HPQ', 75, 33.2),\n",
    "]\n",
    "stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data in this form, it is relatively straightforward to interact with a relational database using Python’s standard database API, as described in PEP [249](https://www.python.org/dev/peps/pep-0249/).  \n",
    "If you want to take a closer look at interfacing with databases, check out the [Database Topic Guide](https://wiki.python.org/moin/DatabaseProgramming)\n",
    "The gist of the API is that all operations on the database are carried out by SQL queries.  \n",
    "Each row of input or output data is represented by a tuple.  \n",
    "To illustrate, you can use the `sqlite3` module that comes with Python.  \n",
    "If you are using a different database like MySql, Postgres, or ODBC, you’ll have to install a third-party module to support it.  \n",
    "The first step is to connect to the database.  \n",
    "You can start by calling the `connect()` function, supplying parameters such as the name of the database, hostname, username, password, and other details as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db = sqlite3.connect('database.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create a cursor and begin working with the data.  \n",
    "Let's execute a few SQL queries."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c = db.cursor()\n",
    "c.execute('create table portfolio (symbol text, shares integer, price real)')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert a sequence of rows into the data, use a statement like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c.executemany('insert into portfolio values (?, ?, ?)', stocks)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a query, use a statement like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n",
      "('FB', 150, 7.45)\n",
      "('HPQ', 75, 33.2)\n"
     ]
    }
   ],
   "source": [
    "for row in db.execute('select * from portfolio'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to perform queries that accept user-supplied input parameters, make sure you escape the parameters using `?` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', 100, 490.1)\n",
      "('AAPL', 50, 545.75)\n"
     ]
    }
   ],
   "source": [
    "min_price = 100\n",
    "for row in db.execute('select * from portfolio where price >= ?', (min_price,)):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a low level, interacting with a database is an extremely straightforward thing to do.  \n",
    "You simply form SQL statements and feed them to the underlying module to either update the database or retrieve data.  \n",
    "That said, there are still some tricky details you’ll need to sort out on a case-by-case basis.  \n",
    "One complication is the mapping of data from the database into Python types.  \n",
    "For entries such as dates, it is most common to use datetime instances from the date time module, or possibly system timestamps, as used in the time module.  \n",
    "For numerical data, especially financial data involving decimals, numbers may be represented as `Decimal` instances from the decimal module.  \n",
    "Unfortunately, the exact mapping varies by database backend so you’ll have to read the associated documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another extremely critical complication concerns the formation of SQL statement strings.  \n",
    "You should never use Python string formatting operators (`%`) or the `.format()` method to create such strings.  \n",
    "If the values provided to such formatting operators are derived from user input, this opens up your program to a(n) [SQL-injection attack](https://xkcd.com/327/).  \n",
    "The special `?` wildcard in queries instructs the database backend to use its own string substitution mechanism, which *should* do it safely.\n",
    "However, there is some inconsistency across database backends with respect to the wildcard.  \n",
    "Many modules use `?` or `%s`, while others may use a different symbol, such as `:0` or `:1`, to refer to parameters.  \n",
    "Again, you’ll have to consult the documentation for the database module you’re using.  \n",
    "The `paramstyle` attribute of a database module also contains information about the quoting style.\n",
    "For simply pulling data in and out of a database table, using the database API is usually simple enough.  \n",
    "If you’re doing something more complicated, it may make sense to use a higher-level interface, such as that provided by an object-relational mapper.  \n",
    "Libraries such as [SQLAlchemy](https://www.sqlalchemy.org/) allow database tables to be described as Python classes and for database operations to be carried out while hiding most of the underlying SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. Decoding and Encoding Hexadecimal Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to decode a string of hexadecimal digits into a byte string or encode a byte string as hex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just need to decode or encode a raw string of hex digits, use the `binascii` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'68656c6c6f'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = b'hello'\n",
    "# Encode as hex\n",
    "import binascii\n",
    "h = binascii.b2a_hex(s)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode to bytes\n",
    "binascii.a2b_hex(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find similar functionality in the `base64` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'68656C6C6F'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "h = base64.b16encode(s)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b16decode(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, converting to and from hex is straightforward using the functions shown.  \n",
    "The main difference between the two techniques is in case folding.  \n",
    "The `base64.b16decode()` and `base64.b16encode()` functions only operate with uppercase hexadecimal letters, whereas the functions in `binascii` work with either case.\n",
    "It’s also important to note that the output produced by the encoding functions is always a byte string.  \n",
    "To coerce it to Unicode for output, you may need to add an extra decoding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'68656C6C6F'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = base64.b16encode(s)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'68656C6C6F'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When decoding hex digits, the `b16decode()` and `a2b_hex()` functions accept either bytes or unicode strings.  \n",
    "However, those strings must only contain ASCII-encoded hexadecimal digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10. Decoding and Encoding Base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to decode or encode binary data using Base64 encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `base64` module has two functions -- `b64encode()` and `b64decode()` -- that do exactly what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ZXhwZXJpbWVudA=='"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = b'experiment'\n",
    "import base64\n",
    "a = base64.b64encode(s)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'experiment'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64decode(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base64 encoding is only meant to be used on byte-oriented data such as byte strings and byte arrays.  \n",
    "Moreover, the output of the encoding process is always a byte string.  \n",
    "If you are mixing Base64-encoded data with Unicode text, you may have to perform an extra decoding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZXhwZXJpbWVudA=='"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = base64.b64encode(s).decode('ascii')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When decoding Base64, both byte strings and Unicode text strings can be supplied.  \n",
    "However, Unicode strings can only contain ASCII characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11. Reading and Writing Binary Arrays of Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to read or write data encoded as a binary array of uniform structures into Python tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with binary data, use the `struct` module.  \n",
    "Here is an example that writes a list of Python tuples to a binary file, encoding each tuple as a structure using `struct`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from struct import Struct \n",
    "\n",
    "def write_records(records, format, f):\n",
    "    \"\"\"\n",
    "    Write a sequence of tuples to a binary file of structures.\n",
    "    \"\"\"\n",
    "    record_struct = Struct(format)\n",
    "    for r in records:\n",
    "        f.write(record_struct.pack(*r))\n",
    "        \n",
    "# An Example:\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    records = [ (1, 2.3, 4.5),\n",
    "                (6, 7.8, 9.0),\n",
    "                (12, 13.4, 56.7) ]\n",
    "    \n",
    "    with open('data.b', 'wb') as f:\n",
    "        write_records(records, '<idd', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several approaches for reading this file back into a list of tuples.  \n",
    "Here is one way to read the file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from struct import Struct\n",
    "\n",
    "def read_records(format, f):\n",
    "    record_struct = Struct(format)\n",
    "    chunks = iter(lambda: f.read(record_struct.size), b'')\n",
    "    return (record_struct.unpack(chunk) for chunk in chunks)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    with open('data.b','rb') as f:\n",
    "    for rec in read_records('<idd', f):\n",
    "        # Process rec\n",
    "        # ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also read the file entirely into a byte string with a single read and convert it piece by piece:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from struct import Struct\n",
    "\n",
    "def unpack_records(format, data):\n",
    "    record_struct = Struct(format)\n",
    "    return (record_struct.unpack_from(data, offset)\n",
    "            for offset in range(0, len(data), record_struct.size))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with open('data.b', 'rb') as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    for rec in unpack_records('<idd', data):\n",
    "        # Process rec\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases the result is an iterable that produces the tuples originally stored when the file was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For programs that must encode and decode binary data, it is common to use the `struct` module.  \n",
    "To declare a new structure, simply create an instance of `Struct` such as:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Little endian 32-bit integer with two double precision floats:\n",
    "record_struct = Struct('<idd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structures are always defined using a set of structure codes such as i, d, f, and so forth.  \n",
    "These codes correspond to specific binary data types such as 32-bit integers, 64-bit floats, 32-bit floats, and so forth.  \n",
    "The `<` in the first character specifies the byte ordering.  \n",
    "In this example, it is indicating little endian.  \n",
    "Change the character to `>` for big endian or `!` for network byte order.  \n",
    "The resulting `Struct` instance has various attributes and methods for manipulating structures of that type.  \n",
    "The size attribute contains the size of the structure in bytes, which is useful to have in I/O operations.  \n",
    "`pack()` and `unpack()` methods are used to pack and unpack data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from struct import Struct\n",
    "record_struct = Struct('<idd')\n",
    "record_struct.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_struct.pack(1, 2.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2.0, 3.0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_struct.unpack(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you’ll see the `pack()` and `unpack()` operations called as module-level functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import struct\n",
    "struct.pack('<idd', 1, 2.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2.0, 3.0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct.unpack('<idd', _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but feels less elegant than creating a single `Struct` instance, especially if the same structure appears in multiple places in your code.  \n",
    "By creating a Struct instance, the format code is only specified once and all of the useful operations are grouped together. This certainly makes it easier to maintain your code if you need to fiddle with the structure code because you only have to change it in one place.\n",
    "The code for reading binary structures involves a number of programming idioms.  \n",
    "In the `read_records()` function, `iter()` is being used to make an iterator that returns fixed-sized chunks.  \n",
    "This iterator repeatedly calls a user-supplied callable (e.g., `lambda: f.read(record_struct.size))` until it returns a specified value (e.g., `b`), at which point iteration stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x10825eb00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data.b', 'rb')\n",
    "chunks = iter(lambda: f.read(20), b'')\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00ffffff\\x02@\\x00\\x00\\x00\\x00\\x00\\x00\\x12@'\n",
      "b'\\x06\\x00\\x00\\x00333333\\x1f@\\x00\\x00\\x00\\x00\\x00\\x00\"@'\n",
      "b'\\x0c\\x00\\x00\\x00\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc*@\\x9a\\x99\\x99\\x99\\x99YL@'\n"
     ]
    }
   ],
   "source": [
    "for chk in chunks:\n",
    "    print(chk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason for creating an iterable is that it nicely allows records to be created using a generator comprehension, as shown in the solution.  \n",
    "If you didn’t use this approach, the code might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_records(format, f):\n",
    "    record_struct = Struct(format)\n",
    "    while True:\n",
    "        chk = f.read(record_struct.size)\n",
    "        if chk == b'':\n",
    "            break\n",
    "        yield record_struct.unpack(chk)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `unpack_records()` function, a different approach using the `unpack_from()` method is used.  \n",
    "`unpack_from()` is a useful method for extracting binary data from a larger binary array, because it does so without making any temporary objects or memory copies.  \n",
    "You just give it a byte string (or any array) along with a byte offset, and it will unpack fields directly from that location.  \n",
    "If you used `unpack()` instead of `unpack_from()`, you would need to modify the code to make a lot of small slices and offset calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_records(format, data):\n",
    "    record_struct = Struct(format)\n",
    "    return (record_struct.unpack(data[offset:offset + record_struct.size])\n",
    "           for offset in range(0, len(data), record_struct.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being more complicated to read, this version also requires a lot more work, as it performs various offset calculations, copies data, and makes small slice objects.  \n",
    "If you’re going to be unpacking a lot of structures from a large byte string you’ve already read, `unpack_from()` is a more elegant approach.  \n",
    "Unpacking records is one place where you might want to use `namedtuple` objects from the `collections` module.  \n",
    "This allows you to set attribute names on the returned tuples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Record = namedtuple('Record', ['kind', 'x', 'y'])\n",
    "\n",
    "with open('data.p', 'rb') as f:\n",
    "    records = (Record(*r) for r in read_records('<idd', f))\n",
    "    \n",
    "for r in records:\n",
    "    print(r.kind, r.x, r.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re writing a program that needs to work with a large amount of binary data, you may be better off using a library such as `numpy`.  \n",
    "For example, instead of reading a binary into a list of tuples, you could read it into a structured array, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 1,  2.3,  4.5), ( 6,  7.8,  9. ), (12, 13.4, 56.7)],\n",
       "      dtype=[('f0', '<i4'), ('f1', '<f8'), ('f2', '<f8')])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = open('data.b', 'rb')\n",
    "records = np.fromfile(f, dtype='<i,<d,<d')\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, if you’re faced with the task of reading binary data in some known file format (i.e., image formats, shape files, HDF5, etc.), check to see if a Python module already exists for it.  \n",
    "There’s no reason to reinvent the wheel if you don’t have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12. Reading Nested and Variable-Sized Binary Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to read complicated binary-encoded data that contains a collection of nested and/or variable-sized records.  \n",
    "Such data might include images, video, shapefiles, and so on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The struct module can be used to decode and encode almost any kind of binary data structure.  \n",
    "To illustrate the kind of data in question here, suppose you have this Python data structure representing a collection of points that make up a series of polygons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = [\n",
    "        [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5) ],\n",
    "        [ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0) ],\n",
    "        [ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2) ],\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose this data was to be encoded into a binary file where the file started with the following header:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Byte | Type   | Description                        |\n",
    "|------|--------|------------------------------------|\n",
    "| 0    | int    | File code (0x1234 little endian)   |\n",
    "| 4    | double | Minimum x (little endian)          |\n",
    "| 12   | double | Minimum y (little endian)          |\n",
    "| 20   | double | Maximum x (little endian           |\n",
    "| 28   | double | Maximum y (little endian)          |\n",
    "| 36   | int    | Number of polygons (little endian) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the header, a series of polygon records follow, each encoded as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Byte | Type   | Description                           |\n",
    "|------|--------|---------------------------------------|\n",
    "| 0    | int    | Record length including len(N bytes)  |\n",
    "| 4-N  | Points | Pairs of (X,Y) coordinates as doubles |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we can build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "import itertools\n",
    "\n",
    "def write_polys(filename, polys):\n",
    "    # Determine bounding box\n",
    "    flattened = list(itertools.chain(*polys))\n",
    "    min_x = min(x for x, y in flattened)\n",
    "    max_x = max(x for x, y in flattened)\n",
    "    min_y = min(y for x, y in flattened)\n",
    "    max_y = max(y for x, y in flattened)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(struct.pack('<iddddi',\n",
    "                           0x1234,\n",
    "                           min_x, min_y,\n",
    "                           max_x, max_y,\n",
    "                           len(polys)))\n",
    "        for poly in polys:\n",
    "            size = len(poly) * struct.calcsize('<dd')\n",
    "            f.write(struct.pack('<i', size+4))\n",
    "            for pt in poly:\n",
    "                f.write(struct.pack('<dd', *pt))\n",
    "                \n",
    "# Call it with our polygon data:\n",
    "write_polys('polys.bin', polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `struct.unpack` function to read back the data.  \n",
    "Basically we reverse the operations performed during writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_polys(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read the header:\n",
    "        header = f.read(40)\n",
    "        file_code, min_x, min_y, max_x, max_y, num_polys = \\\n",
    "            struct.unpack('<iddddi', header)\n",
    "        \n",
    "        polys = []\n",
    "        for n in range(num_polys):\n",
    "            pbytes, = struct.unpack('<i', f.read(4))\n",
    "            poly = []\n",
    "            for m in range(pbytes // 16):\n",
    "                pt = struct.unpack('<dd', f.read(16))\n",
    "                poly.append(pt)\n",
    "            polys.append(poly)\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Although this code works, it’s also a rather messy mix of small reads, struct unpacking, and other details.  \n",
    "If code like this is used to process a real datafile, it can quickly become even worse.  \n",
    "Thus, it’s an obvious candidate for an alternative solution that might simplify some of the steps and free the programmer to focus on more important matters.  \n",
    "In the remainder of this recipe, a rather advanced solution for interpreting binary data will be built up in pieces.  \n",
    "The goal will be to allow a programmer to provide a high-level specification of the file format, and to simply have the details of reading and unpacking all of the data worked out under the covers.  \n",
    "As a forewarning, the code that follows may be the most advanced example in this entire book, utilizing various object-oriented programming and metaprogramming techniques.  \n",
    "Be sure to carefully read the discussion section as well as cross-references to other recipes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, when reading binary data, it is common for the file to contain headers and other data structures.  \n",
    "Although the struct module can unpack this data into a tuple, another way to represent such information is through the use of a class.  \n",
    "Here’s some code that allows just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "class StructField:\n",
    "    \"\"\"\n",
    "    Descriptor that represents a simple structure field\n",
    "    \"\"\"\n",
    "    def __init__(self, format, offset):\n",
    "        self.format = format\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            r = struct.unpack_from(self.format, instance._buffer, self.offset)\n",
    "            return r[0] if len(r) == 1 else r\n",
    "\n",
    "class Structure:\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = memoryview(bytedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses a descriptor to represent each structure field.  \n",
    "Each descriptor contains a `struct`-compatible format code along with a byte offset into an underlying memory buffer.  \n",
    "In the `__get__()` method, the `struct.unpack_from()` function is used to unpack a value from the buffer without having to make extra slices or copies.  \n",
    "The `Structure` class just serves as a base class that accepts some byte data and stores it as the underlying memory buffer used by the `StructField` descriptor.  \n",
    "The use of a `memoryview()` in this class serves a purpose that will become clear later.  \n",
    "Using this code, you can now define a structure as a high-level class that mirrors the information found in the tables that described the expected file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyHeader(Structure):\n",
    "    file_code = StructField('<i', 0)\n",
    "    min_x = StructField('<d', 4)\n",
    "    min_y = StructField('<d', 12)\n",
    "    max_x = StructField('<d', 20)\n",
    "    max_y = StructField('<d', 28)\n",
    "    num_polys = StructField('<i', 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this class to read the header from the polygon data written earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('polys.bin', 'rb')\n",
    "phead = PolyHeader(f.read(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.file_code == 0x1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.num_polys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, but there are a number of annoyances with this approach.  \n",
    "For one, even though you get the convenience of a class-like interface, the code is rather verbose and requires the user to specify a lot of low-level detail with repeated uses of Struct Field and specification of offsets.  \n",
    "The resulting class is also missing common conveniences such as providing a way to compute the total size of the structure.  \n",
    "Any time you are faced with class definitions that are overly verbose like this, you might consider the use of a class decorator or metaclass.  \n",
    "One of the features of a metaclass is that it can be used to fill in a lot of low-level implementation details, taking that burden off of the user.  \n",
    "As an example, consider this metaclass and slight reformulation of the `Structure` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureMeta(type):\n",
    "    \"\"\"\n",
    "        Metaclass that automatically creates StructField descriptors.\n",
    "    \"\"\"\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        for format, fieldname in fields:\n",
    "            if format.startswith(('<','>','!','@')):\n",
    "                byte_order = format[0]\n",
    "                format = format[1:]\n",
    "            format = byte_order + format\n",
    "            setattr(self, fieldname, StructField(format, offset))\n",
    "            offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)\n",
    "        \n",
    "class Structure(metaclass=StructureMeta):\n",
    "    def __init__(self, bytedata):\n",
    "        self._buffer = bytedata\n",
    "        \n",
    "    @classmethod\n",
    "    def from_file(cls, f):\n",
    "        return cls(f.read(cls.struct_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this new `Structure` class, you can now write a structure definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyHeader(Structure):\n",
    "    _fields_ = [\n",
    "        ('<i', 'file_code'),\n",
    "        ('d', 'min_x'),\n",
    "        ('d', 'min_y'),\n",
    "        ('d', 'max_x'),\n",
    "        ('i', 'num_polys')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the specification is a lot less verbose.  \n",
    "The added `from_file()` class method also makes it easier to read the data from a file without knowing any details about the size or structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('polys.bin', 'rb')\n",
    "phead = PolyHeader.from_file(f)\n",
    "phead.file_code == 0x1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.max_x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "phead.max_y\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "<ipython-input-101-192b26193ac4> in <module>()\n",
    "----> 1 phead.max_y\n",
    "\n",
    "AttributeError: 'PolyHeader' object has no attribute 'max_y'\n",
    "\n",
    "????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717986918"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phead.num_polys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you introduce a metaclass into the mix, you can build more intelligence into it. \n",
    "For example, suppose you want to support nested binary structures.  \n",
    "Here's a reformulation of the metaclass along with a new supporting descriptor that allows it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedStruct:\n",
    "    \"\"\"\n",
    "    Descriptor that represents a nested structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, struct_type, offset):\n",
    "        self.name = name\n",
    "        self.struct_type = struct_type\n",
    "        self.offset = offset\n",
    "    def __get__(self, instance, cls):\n",
    "        if instance is None:\n",
    "            return self\n",
    "        else:\n",
    "            data = instance._buffer[self.offset: self.offset+self.struct_type.struct_size]\n",
    "            result = self.struct_type(data)\n",
    "            # Save the resulting structure for later:\n",
    "            setattr(instance, self.name, result)\n",
    "            return result\n",
    "        \n",
    "class StructureMeta(type):\n",
    "    \"\"\"\n",
    "    This metaclass automatically creates StructField descriptors.\n",
    "    \"\"\"\n",
    "    def __init__(self, clsname, bases, clsdict):\n",
    "        fields = getattr(self, '_fields_', [])\n",
    "        byte_order = ''\n",
    "        offset = 0\n",
    "        for format, fieldname in fields:\n",
    "            if isinstance(format, StructureMeta):\n",
    "                setattr(self, fieldname, NestedStruct(fieldname, format, offset))\n",
    "                offset += format.struct_size\n",
    "            else:\n",
    "                if format.startswith(('<','>','!','@')):\n",
    "                    byte_order = format[0]\n",
    "                    format = format[1:]\n",
    "                format = byte_order + format\n",
    "                setattr(self, fieldname, StructField(format, offset))\n",
    "                offset += struct.calcsize(format)\n",
    "        setattr(self, 'struct_size', offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the `NestedStruct` descriptor is used to overlay another structure definition over a region of memory.  \n",
    "It does this by taking a slice of the original memory buffer and using it to instantiate the given structure type.  \n",
    "Since the underlying memory buffer was initialized as a memoryview, this slicing does not create a copy in a different memory address.  \n",
    "Instead, it's placed on top of the original memory.  \n",
    "In order to avoid repeated instantiations, the descriptor then stores the resulting inner structure object on the instance using the same technique described in Recipe 8.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the code for our `Point` and `PolyHeader` classes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Point(Structure):\n",
    "    _fields_ = [\n",
    "        ('<d', 'x'),\n",
    "        ('d', 'y')\n",
    "    ]\n",
    "\n",
    "class PolyHeader(Structure):\n",
    "    _fields_ = [\n",
    "        ('<i', 'file_code'),\n",
    "        (Point, 'min'), # nested struct\n",
    "        (Point, 'max'), # nested struct\n",
    "        ('i', 'num_polys')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, a framework for dealing with fixed-sized records has been developed, but what about the variable-sized components?  \n",
    "For example, the remainder of the polygon files contain sections of variable size.  \n",
    "One way to handle this is to write a class that simply represents a chunk of binary data along with a utility function for interpreting the contents in different ways.  \n",
    "This is closely related to the code in Recipe 6.11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
