{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6  \n",
    "# Data Encoding and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main focus of this chapter is using Python to process data presented in different kinds of common encodings, such as CSV files, JSON, XML, and binary packed records.  \n",
    "Unlike the chapter on data structures, this chapter is not focused on specific algorithms, but instead on the problem of getting data in and out of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Reading and Writing CSV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read or write data encoded as a CSV file, you can use Python's `csv` library.  \n",
    "We will use some stock market data from a CSV file for this example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stocks.csv\n",
    "\n",
    "Symbol,Price,Date,Time,Change,Volume\n",
    "\"AA\",39.48,\"6/11/2007\",\"9:36am\",-0.18,181800\n",
    "\"AIG\",71.38,\"6/11/2007\",\"9:36am\",-0.15,195500\n",
    "\"AXP\",62.58,\"6/11/2007\",\"9:36am\",-0.46,935000\n",
    "\"BA\",98.31,\"6/11/2007\",\"9:36am\",+0.12,104800\n",
    "\"C\",53.08,\"6/11/2007\",\"9:36am\",-0.25,360900\n",
    "\"CAT\",78.29,\"6/11/2007\",\"9:36am\",-0.23,225400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the data as a sequence of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # Process row\n",
    "        # ... and so forth\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, `row` will be a tuple.  \n",
    "Thus, to access certain fields, you will need to use indexing, such as `row[0]` (Symbol) and `row[4]` (Change).  \n",
    "Since such indexing can often be confusing, this is one place where you might want to consider the use of named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headings = next(f_csv)\n",
    "    Row = namedtuple('Row', headings)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        # Process row\n",
    "        # ... and so forth\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would allow you to use the column headers such as `row.Symbol` and `row.Change` instead of indices.  \n",
    "It should be noted that this only works if the column headers are valid Python identifiers.  \n",
    "If not, you might have to massage the initial headings (e.g., replacing nonidentifier characters with underscores or similar).  \n",
    "Another approach allows you to read the data as a sequence of dictionaries instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.DictReader(f)\n",
    "    for row in f_csv:\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, youo would access the elements of each row using the row headers.  \n",
    "For example, `row['Symbol']` or `row['Change']`.  \n",
    "To write CSV data, you also use the `csv` module, but you create a writer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol','Price','Date','Time','Change','Volume']\n",
    "rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800),\n",
    "            ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500),\n",
    "            ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv', 'w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the data as a sequence of dictionaries, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\n",
    "rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.18, 'Volume':181800},\n",
    "        {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.15, 'Volume': 195500},\n",
    "        {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007',\n",
    "         'Time':'9:36am', 'Change':-0.46, 'Volume': 935000},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv', 'w') as f:\n",
    "    f_csv = csv.DictWriter(f, headers)\n",
    "    f_csv.writeheader()\n",
    "    f_csv.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `csv` module can save you quite a bit of time over parsing, splitting, and cleaning the data manually by yourself.  \n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv') as f:\n",
    "    for line in f:\n",
    "        row = line.split(',')\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that you’ll still need to deal with some nasty details.  \n",
    "For example, if any of the fields are surrounded by quotes, you’ll have to strip the quotes.  \n",
    "In addition, if a quoted field happens to contain a comma, the code will break by producing a row with the wrong size.  \n",
    "By default, the `csv` library is programmed to understand CSV encoding rules used by Microsoft Excel.  \n",
    "This is probably the most common variant, and will likely give you the best compatibility.  \n",
    "However, if you consult the documentation for csv, you’ll see a few ways to tweak the encoding to different formats (e.g., changing the separator character, etc.).  \n",
    "For example, if you want to read tab-delimited data instead, use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stocks.csv') as f:\n",
    "    f_tsv = csv.reader(f, delimiter='\\t')\n",
    "    for row in f_tsv:\n",
    "        # Do something ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're reading CSV data and converting it into named tuples, use caution when validating column headers.  \n",
    "For example, a CSV file could have a header line containing nonvalid identifier characters like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Street Address,Num-Premises,Latitude,Longitude`  \n",
    "`5412 N CLARK,10,41.980262,-87.668452`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will actually cause the creation of a `namedtuple` to fail with a `ValueError` exception.  \n",
    "To work around this, you might have to scrub the headers first.  \n",
    "For instance, carrying a regex substitution on nonvalid identifier characters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]\n",
    "    Row = namedtuple('Row', headers)\n",
    "    for r in f_csv:\n",
    "        row = Row(*r)\n",
    "        # do something\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that `csv` does not try to interpret the data or convert it to a type other than a string.  \n",
    "The following example performs extra type conversions on CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = [str, float, str, str, float, int]\n",
    "with open('stocks.csv') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    headers = next(f_csv)\n",
    "    for row in f_csv:\n",
    "        # Apply conversions to the row items\n",
    "        row = tuple(convert(value) for convert, value in zip(col_types, row))\n",
    "        # And so forth ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert selected fields of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading as dicts with type conversion\n",
      "OrderedDict([('Symbol', 'AA'), ('Price', 39.48), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.18), ('Volume', 181800)])\n",
      "OrderedDict([('Symbol', 'AIG'), ('Price', 71.38), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.15), ('Volume', 195500)])\n",
      "OrderedDict([('Symbol', 'AXP'), ('Price', 62.58), ('Date', '6/11/2007'), ('Time', '9:36am'), ('Change', -0.46), ('Volume', 935000)])\n"
     ]
    }
   ],
   "source": [
    "print('Reading as dicts with type conversion')\n",
    "field_types = [ ('Price', float),\n",
    "                ('Change', float),\n",
    "                ('Volume', int) ]\n",
    "\n",
    "with open('stocks.csv') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row.update((key, conversion(row[key])) for key, conversion in field_types)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you’ll probably want to be a bit careful with such conversions, though.  \n",
    "In the real world, it’s common for CSV files to have missing values, corrupted data, and other issues that would break type conversions.  \n",
    "So, unless your data is guaranteed to be error free, that’s something you’ll need to consider (you might need to add suitable exception handling).  \n",
    "Finally, if your goal in reading CSV data is to perform data analysis and statistics, you might want to look at the `pandas` package.  \n",
    "`pandas` includes a convenient `pandas.read_csv()` function that will load CSV data into a `DataFrame` object.  \n",
    "From there, you can generate various summary statistics, filter the data, and perform other kinds of high-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Reading and Writing JSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem  \n",
    "You want to read or write data encoded as JavaScript Object Notation (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution  \n",
    "The `json` module provides an easy way to encode and decode data in JSON.  \n",
    "The two main functions are `json.dumps()` and `json.loads()`, mirroring the interface used in other serialization libraries, such as `pickle`.  \n",
    "Here is how you turn a Python data structure into JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"ACME\", \"shares\": 100, \"price\": 542.23}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    'name' : 'ACME',\n",
    "    'shares' : 100,\n",
    "    'price': 542.23\n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn the JSON-encoded string back into a Python data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ACME', 'shares': 100, 'price': 542.23}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(json_str); data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with files instead of strings, you can also use `json.dump()` and `json.load()` to encode and decode JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ACME', 'shares': 100, 'price': 542.23}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the data\n",
    "with open ('data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "    \n",
    "# Read data back\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion  \n",
    "JSON encoding supports the basic types of `None, bool, int, float,` and `str`, as well as lists, tuples, and dictionaries containing those types.  \n",
    "For dictionaries, keys are assumed to be strings (any non-string keys in a dictionary are converted to strings during encoding).  \n",
    "To be compliant with the JSON specification, you should only encode Python lists and dictionaries.  \n",
    "Note that in web applications, it is also conventional for the top-level object to be a dictionary.  \n",
    "The format of JSON encoding is almost identical to Python syntax except for a few minor changes.  \n",
    "for instance, `True` is mapped to `true`, `False` is mapped to `false`, and `None` is mapped to `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"a\": true, \"b\": \"Hello\", \"c\": null}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'a' : True,\n",
    "    'b' : 'Hello',\n",
    "    'c': None\n",
    "}\n",
    "\n",
    "json.dumps(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are trying to examine data you have decoded from JSON, it can often be hard to ascertain its structure simply by printing it out, especially if the data contains a deep level of nested structures or a lot of fields.  \n",
    "To assist with this, consider using the `pprint()` function in the pprint module.  \n",
    "This will alphabetize the keys and output a dictionary in a more sane way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, JSON decoding will create dicts or lists from the supplied data.  \n",
    "If you want to create different kinds of objects, supply the `object_pairs_hook` or `object_hook` to `json.loads()`.  \n",
    "Here is one way you can encode JSON data that preserves its order in an `OrderedDict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "data = json.loads(s, object_pairs_hook=OrderedDict); data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also turn a JSON dictionary into a Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ACME', 50, 490.1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JSONObject:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "        \n",
    "data = json.loads(s, object_hook=JSONObject)\n",
    "data.name, data.shares, data.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, the dictionary created by decoding the JSON data is passed as a single argument to `__init__()`.  \n",
    "From there, you can use it directly as the instance dictionary of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few options that can be useful for encoding JSON.  \n",
    "If you would like the output to be nicely formatted, you can use the indent argument to `json.dumps()`.  \n",
    "This causes the output to be pretty printed in a format similar to that with the `pprint()` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"ACME\", \"shares\": 100, \"price\": 542.23}\n",
      "{\n",
      "    \"name\": \"ACME\",\n",
      "    \"shares\": 100,\n",
      "    \"price\": 542.23\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(json.dumps(data))\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `sort_keys` argument to sort the keys alphabetically on output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"ACME\", \"price\": 542.23, \"shares\": 100}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances are not normally serializable as JSON.  \n",
    "The following code breaks down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "p = Point(2, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json.dumps(p)\n",
    "\n",
    ">>>>>>>\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-25-e93da69829e6> in <module>()\n",
    "      5 \n",
    "      6 p = Point(2, 3)\n",
    "----> 7 json.dumps(p)\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n",
    "    229         cls is None and indent is None and separators is None and\n",
    "    230         default is None and not sort_keys and not kw):\n",
    "--> 231         return _default_encoder.encode(obj)\n",
    "    232     if cls is None:\n",
    "    233         cls = JSONEncoder\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in encode(self, o)\n",
    "    197         # exceptions aren't as detailed.  The list call should be roughly\n",
    "    198         # equivalent to the PySequence_Fast that ''.join() would do.\n",
    "--> 199         chunks = self.iterencode(o, _one_shot=True)\n",
    "    200         if not isinstance(chunks, (list, tuple)):\n",
    "    201             chunks = list(chunks)\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)\n",
    "    255                 self.key_separator, self.item_separator, self.sort_keys,\n",
    "    256                 self.skipkeys, _one_shot)\n",
    "--> 257         return _iterencode(o, 0)\n",
    "    258 \n",
    "    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
    "\n",
    "~/.pyenv/versions/3.6.1/lib/python3.6/json/encoder.py in default(self, o)\n",
    "    178         \"\"\"\n",
    "    179         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n",
    "--> 180                         o.__class__.__name__)\n",
    "    181 \n",
    "    182     def encode(self, o):\n",
    "\n",
    "TypeError: Object of type 'Point' is not JSON serializable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to serialize instances, you can supply a function that takes an instance as input and returns a dictionary that can be serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_instance(obj):\n",
    "    d = { '__classname__' : type(obj).__name__ }\n",
    "    d.update(vars(obj))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an instance back, you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping names to known classes\n",
    "classes = { 'Point' : Point }\n",
    "\n",
    "def unserialize_object(d):\n",
    "    clsname = d.pop('__classname__', None)\n",
    "    if clsname:\n",
    "        cls = classes[clsname]\n",
    "        obj = cls.__new__(cls)  # Creates an instance without calling the __init__() method\n",
    "        for key, value in d.items():\n",
    "            setattr(obj, key, value)\n",
    "            return obj\n",
    "    else:\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"__classname__\": \"Point\", \"x\": 2, \"y\": 3}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Point(2,3)\n",
    "s = json.dumps(p, default=serialize_instance); s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Point at 0x110499d68>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = json.loads(s, object_hook=unserialize_object); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `json` module has a variety of other options for controlling the low-level interpretation of numbers, special values such as `NaN`, and more.  \n",
    "[The JavaScript Object Notation (JSON) Data Interchange Format](https://tools.ietf.org/html/rfc8259)  \n",
    "[`json` — JSON encoder and decoder](https://docs.python.org/3.7/library/json.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Parsing Simple XML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xml.etree.ElementTree` module can be used to extract data from simple XML documents.  \n",
    "To illustrate, suppose you want to parse and make a summary of the RSS feed on [Planet Python](https://planetpython.org/).  \n",
    "The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x110499a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "# Download the RSS feed and parse it:\n",
    "u = urlopen('https://planet.python.org/rss20.xml')\n",
    "doc = parse(u); doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract and output the tags that interest us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Driscoll: Jupyter Notebook Extension Basics\n",
      "Tue, 02 Oct 2018 05:05:08 +0000\n",
      "http://www.blog.pythonlibrary.org/2018/10/02/jupyter-notebook-extension-basics/\n",
      "\n",
      "Kay Hayen: Nuitka this week #8\n",
      "Tue, 02 Oct 2018 04:05:00 +0000\n",
      "http://nuitka.net/posts/nuitka-this-week-8.html\n",
      "\n",
      "Podcast.__init__: Managing Application Secrets with Brian Kelly\n",
      "Tue, 02 Oct 2018 02:12:21 +0000\n",
      "https://www.podcastinit.com/managing-application-secrets-with-brian-kelly-episode-181/\n",
      "\n",
      "Anarcat: October 2018 report: LTS, Mastodon, Firefox privacy, etc\n",
      "Mon, 01 Oct 2018 20:28:22 +0000\n",
      "https://anarc.at/blog/2018-10-01-report/\n",
      "\n",
      "Bill Ward / AdminTome: Install Python 3.7.0 on Ubuntu 18.04 / Debian 9.5\n",
      "Mon, 01 Oct 2018 18:37:19 +0000\n",
      "https://www.admintome.com/blog/install-python-3-7-0-on-ubuntu-18-04/\n",
      "\n",
      "Bruno Rocha: Hacktoberfest 2018\n",
      "Mon, 01 Oct 2018 18:20:20 +0000\n",
      "http://brunorocha.org/hacktoberfest-2018.html\n",
      "\n",
      "Made With Mu: PyWeek - Make a Game with Mu\n",
      "Mon, 01 Oct 2018 18:00:00 +0000\n",
      "https://madewith.mu/mu/games/2018/10/01/pyweek26.html\n",
      "\n",
      "Tryton News: Tryton Release 5.0\n",
      "Mon, 01 Oct 2018 16:00:00 +0000\n",
      "https://discuss.tryton.org/t/tryton-release-5-0/711\n",
      "\n",
      "Curtis Miller: My Tutorial Book on Anaconda, NumPy and Pandas Is Out: Hands-On Data Analysis with NumPy and Pandas\n",
      "Mon, 01 Oct 2018 15:00:57 +0000\n",
      "\n",
      "\n",
      "Real Python: Splitting, Concatenating, and Joining Strings in Python\n",
      "Mon, 01 Oct 2018 14:00:00 +0000\n",
      "https://realpython.com/python-string-split-concatenate-join/\n",
      "\n",
      "PyBites: Code Challenge 53 - Query the Spotify API\n",
      "Mon, 01 Oct 2018 13:45:27 +0000\n",
      "https://pybit.es/codechallenge53.html\n",
      "\n",
      "PyBites: Code Challenge 52 - Create your own Pomodoro Timer - Review\n",
      "Mon, 01 Oct 2018 12:40:00 +0000\n",
      "https://pybit.es/codechallenge52_review.html\n",
      "\n",
      "Made With Mu: Announcing Mu version 1.0.1\n",
      "Mon, 01 Oct 2018 11:00:00 +0000\n",
      "https://madewith.mu/mu/releases/2018/10/01/mu-1-0-1.html\n",
      "\n",
      "Django Weblog: Django security release issued: 2.1.2\n",
      "Mon, 01 Oct 2018 09:26:35 +0000\n",
      "https://www.djangoproject.com/weblog/2018/oct/01/security-release/\n",
      "\n",
      "Django Weblog: Django bugfix releases: 2.0.9 and 1.11.16\n",
      "Mon, 01 Oct 2018 09:26:26 +0000\n",
      "https://www.djangoproject.com/weblog/2018/oct/01/bugfix-releases/\n",
      "\n",
      "Julien Danjou: Code Style Checks in Python\n",
      "Mon, 01 Oct 2018 09:00:00 +0000\n",
      "https://julien.danjou.info/code-style-checks-in-python/\n",
      "\n",
      "Tryton News: Release 0.8.1 of relatorio\n",
      "Mon, 01 Oct 2018 08:00:02 +0000\n",
      "https://discuss.tryton.org/t/release-0-8-1-of-relatorio/775\n",
      "\n",
      "Tryton News: Release 1.0.0 of python-sql\n",
      "Mon, 01 Oct 2018 06:00:04 +0000\n",
      "https://discuss.tryton.org/t/release-1-0-0-of-python-sql/774\n",
      "\n",
      "Python Celery - Weekly Celery Tutorials and How-tos: Celery, docker and the missing startup banner\n",
      "Mon, 01 Oct 2018 06:00:00 +0000\n",
      "https://www.distributedpython.com/2018/10/01/celery-docker-startup/\n",
      "\n",
      "Mike Driscoll: PyDev of the Week: Jacqueline Kazil\n",
      "Mon, 01 Oct 2018 05:05:54 +0000\n",
      "http://www.blog.pythonlibrary.org/2018/10/01/pydev-of-the-week-jacqueline-kazil/\n",
      "\n",
      "Evennia: Evennia 0.8 released\n",
      "Sun, 30 Sep 2018 21:35:23 +0000\n",
      "http://evennia.blogspot.com/2018/09/evennia-08-released.html\n",
      "\n",
      "Nikola: Nikola v8.0.1 is out!\n",
      "Sun, 30 Sep 2018 17:53:13 +0000\n",
      "https://getnikola.com/blog/nikola-v801-is-out.html\n",
      "\n",
      "Weekly Python StackOverflow Report: (cxlv) stackoverflow python report\n",
      "Sun, 30 Sep 2018 08:57:00 +0000\n",
      "http://python-weekly.blogspot.com/2018/09/cxlv-stackoverflow-python-report.html\n",
      "\n",
      "REPL|REBL: Dictionary Views &amp; Set Operations — Working with dictionary view objects\n",
      "Sun, 30 Sep 2018 06:00:00 +0000\n",
      "https://www.pymadethis.com/article/python-dictionary-sets/\n",
      "\n",
      "Gocept Weblog: Saltlabs Sprint: last minute information\n",
      "Sat, 29 Sep 2018 13:16:10 +0000\n",
      "https://blog.gocept.com/2018/09/28/saltlabs-sprint-last-minute-information/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in doc.iterfind('channel/item'):\n",
    "    title = item.findtext('title')\n",
    "    date = item.findtext('pubDate')\n",
    "    link = item.findtext('link')\n",
    "    print(title)\n",
    "    print(date)\n",
    "    print(link)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data encoded as XML is commonplace in many applications.  \n",
    "Not only is XML widely used as a format for exchanging data on the Internet, it is a common format for storing application data (e.g., word processing, music libraries, etc.).  \n",
    "The discussion that follows already assumes the reader is familiar with XML basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, when XML is simply being used to store data, the document structure is compact and straightforward.  \n",
    "The `xml.etree.ElementTree.parse()` function parses the entire XML document into a document object.  \n",
    "From there, you use methods such as `find()`, `iterfind()`, and `findtext()` to search for specific XML elements.  \n",
    "The arguments to these functions are the names of a specific tag, such as channel/item or title.\n",
    "When specifying tags, you need to take the overall document structure into account.  \n",
    "Each find operation takes place relative to a starting element. \n",
    "Likewise, the tagname that you supply to each operation is also relative to the start.  \n",
    "In the example, the call to `doc.iterfind('channel/item')` looks for all \"item\" elements under a \"channel\" element. doc represents the top of the document (the top-level \"rss\" element).  \n",
    "The later calls to `item.findtext()` take place relative to the found \"item\" elements.  \n",
    "Each element represented by the `ElementTree` module has a few essential attributes and methods that are useful when parsing.  \n",
    "The tag attribute contains the name of the tag, the text attribute contains enclosed text, and the `get()` method can be used to extract attributes (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x110499a90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'title' at 0x1104cfea8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = doc.find('channel/title'); e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planet Python'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that `xml.etree.ElementTree` is not the only option for XML parsing.  \n",
    "For more advanced applications, you might consider `lxml`.  \n",
    "It uses the same program‐ ming interface as ElementTree, so the example shown in this recipe works in the same manner.  \n",
    "You simply need to change the first import to:  \n",
    "`from lxml.etree import parse`.  \n",
    "`lxml` provides the benefit of being fully compliant with XML standards.  \n",
    "It is also extremely fast, and provides support for features such as validation, XSLT, and XPath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Parsing Huge XML Files Incrementally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
