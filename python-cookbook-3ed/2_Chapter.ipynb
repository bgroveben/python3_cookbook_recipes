{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 2. Strings and Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost every useful program involves some kind of processing, whether it is parsing data or generating output.  \n",
    "This chapter focuses on common problems involving text manipulation, such as pulling apart strings, searching, substitution, lexing, and parsing.  \n",
    "Many of these tasks can be easily solved using built-in methods of strings.  \n",
    "However, more complicated operations might require the use of regular expressions or the creation of a full-fledged parser.  \n",
    "All of those topics will be covered.  \n",
    "In addition, a few tricky aspects of [working with Unicode](https://en.wikipedia.org/wiki/Unicode) are addressed.  \n",
    "Let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Splitting Strings on Any of Multiple Delimiters](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_splitting_strings_on_any_of_multiple_delimiters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to split a string into fields, but the delimiters (and spacing around them) aren't consistent throughout the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split()` method of string objects is really meant for very simple cases, and does not allow for multiple delimiters or account for possible whitespace around the delimiters.  \n",
    "In cases when you need a bit more flexibility, use the [re.split() method](https://docs.python.org/3/library/re.html#re.split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "line = 'asdf fjdk; afed, fjek,asdf,      foo'\n",
    "re.split(r'[;,\\s]\\s*', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some [examples from the documentation](https://docs.python.org/3/library/re.html#re.split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words', 'words', '']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W+', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', ', ', 'words', ', ', 'words', '.', '']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('(\\W+)', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words, words.']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W+', 'Words, words, words.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '3', '9']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[a-f]+', '0a3B9', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `re.split()` function is useful because you can specify multiple patterns for the separator.  \n",
    "For example, as shown in the solution, the separator is either a comma (,), semicolon (;), or whitespace followed by any amount of extra whitespace.  \n",
    "Whenever that pattern is found, the entire match becomes the delimiter between whatever fields lie on either side of the match.  \n",
    "The result is a list of fields, just as with `str.split()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `re.split()`, you need to be a bit careful should the regular expression pattern involve a capture group enclosed in parentheses.  \n",
    "If capture groups are used, then the matched text is also included in the result.  \n",
    "For example, watch what happens here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'asdf fjdk; afed, fjek,asdf,      foo'\n",
    "fields = re.split(r'(;|,|\\s)\\s*', line)\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the split characters might be useful in certain contexts.  \n",
    "For example, maybe you need the split characters later on to reform an output string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = fields[::2]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ';', ',', ',', ',', '']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiters = fields[1::2] + ['']\n",
    "delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdf fjdk;afed,fjek,asdf,foo'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now re-form the line using the same delimiters and values:\n",
    "''.join(v+d for v, d in zip(values, delimiters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdf fjdk; afed, fjek,asdf,      foo'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original line for reference:\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want the separator characters in the result, but still need to use parentheses to group parts of the regular expression pattern, make sure you use a [noncapture group](https://stackoverflow.com/questions/3512471/what-is-a-non-capturing-group-what-does-a-question-mark-followed-by-a-colon), specified as `(?:...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?:,|;|\\s)\\s*', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Matching Text at the Start or End of a String](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_matching_text_at_the_start_or_end_of_a_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to check the start or end of a string for specific text patterns, such as filename extensions, URL schemes, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to check the beginning or end of a string is to use the `str.startswith()` or `str.endswith()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'spam.txt'\n",
    "filename.endswith('.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename.startswith('file:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.python.org'\n",
    "url.startswith('http:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check against multiple choices, simply provide a tuple of possibilities to `startswith()` or `endswith()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '1_Chapter.ipynb',\n",
       " '2_Chapter.ipynb',\n",
       " 'python_ipsum.txt']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filenames = os.listdir('.')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_Chapter.ipynb', '2_Chapter.ipynb', 'python_ipsum.txt']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in filenames if name.endswith(('.ipynb', '.txt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(name.endswith('.ipynb') for name in filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, for your edutainment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "def read_data(name):\n",
    "    if name.startswith(('http:', 'https:', 'ftp:')):\n",
    "        return urlopen(name).read()\n",
    "    else:\n",
    "        with open(name) as f:\n",
    "            return f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddly, this is one part of Python where a tuple is actually required as input.  \n",
    "If you happen to have the choices specified in a list or set, just make sure that you convert them to a tuple  (using `tuple()`) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choices = ['http:', 'ftp']\n",
    "url = 'http://www.python.org'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url.startswith(choices)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-24-618fcafc4cd7> in <module>()\n",
    "----> 1 url.startswith(choices)\n",
    "\n",
    "TypeError: startswith first arg must be str or a tuple of str, not list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url.startswith(tuple(choices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `startswith()` and `endswith()` methods provide a very convenient way to perform basic prefix and suffix checking.  \n",
    "Similar operations can be performed with slices, but are far less elegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'spam.txt'\n",
    "filename[-4:] == '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.python.org'\n",
    "url[:5] == 'http:' or url[:6] == 'https:' or url[:4] == 'ftp:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also be inclined to use regular expressions as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 5), match='http:'>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = 'http://www.python.org'\n",
    "re.match('http:|https:|ftp:', url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but it is often overkill for simple matching.  \n",
    "Using the str.startswith() or str.endswith() methods is simpler and much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, the `startswith()` and `endswith()` methods look nice when combined with other operations, such as common data reductions.  \n",
    "In the next example, this statement checks a directory for the presence of certain kinds of files:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if any(name.endswith(('.ipynb', '.txt')) for name in listdir(dirname)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Matching Strings Using Shell Wildcard Patterns](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_matching_strings_using_shell_wildcard_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to match text using the same wildcard patterns that are commonly used when working with Unix shells (e.g., `*.py, Dat[0-9]*.csv`, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fnmatch` module provides two functions -- `fnmatch()` and `fnmatchcase()` -- that can be used to perform such matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_ipsum.txt\n"
     ]
    }
   ],
   "source": [
    "# Here's an example from the Python 3 documentation:\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "\n",
    "for file in os.listdir('.'):\n",
    "    if fnmatch.fnmatch(file, '*.txt'):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fnmatch import fnmatch, fnmatchcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnmatch('foo.txt', '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnmatch('foo.txt', '?oo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnmatch('Dat45.csv', 'Dat[0-9]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dat1.csv', 'Dat2.csv']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']\n",
    "[name for name in names if fnmatch(name, 'Dat*.csv')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, `fnmatch()` matches patterns using the same case-sensitivity rules as the system's undelying filesystem (which varies based on the operating system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On Mac OSX:\n",
    "fnmatch('foo.txt', '*.TXT')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# On Windows:\n",
    "fnmatch('foo.txt', '*.TXT')\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this distinction matters, use `fnmatchcase()` instead.  \n",
    "It matches exactly based on the lower- and upper-case conventions that you supply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnmatchcase('foo.txt', '*.TXT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often overlooked feature of these functions is their potential use with data processing of nonfilename strings.  \n",
    "For example, suppose that you have a list of street addresses like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses = [\n",
    "    '5412 N CLARK ST',\n",
    "    '1060 W ADDISON ST',\n",
    "    '1039 W GRANVILLE AVE',\n",
    "    '2122 N CLARK ST',\n",
    "    '4802 N BROADWAY',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could write list comprehensions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fnmatch import fnmatchcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5412 N CLARK ST', '1060 W ADDISON ST', '2122 N CLARK ST']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[addr for addr in addresses if fnmatchcase(addr, '* ST')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5412 N CLARK ST']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[addr for addr in addresses if fnmatchcase(addr, '54[0-9][0-9] *CLARK*')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matching performed by `fnmatch` sits somewhere between the functionality of simple string methods and the full power of regular expressions.  \n",
    "If you're just trying to provide a simple mechanism for allowing wildcards in data processing operations, it's often a reasonable solution.  \n",
    "If you're actually trying to write code that matches filenames, use the `glob` module instead.  \n",
    "See [\"Getting A Directory Listing\"](http://chimera.labs.oreilly.com/books/1230000000393/ch05.html#dirlisting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Matching and Searching for Text Patterns](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_matching_and_searching_for_text_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to match or search text for a specific pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text you're trying to match is a simple literal, you can often just use the basic string methods, such as `str.find(), str.endswith(), str.startswith(),` or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'yeah, but no, but yeah, but no, but yeah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exact match:\n",
    "text == 'yeah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match at the beginning:\n",
    "text.startswith('yeah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match at the end:\n",
    "text.endswith('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Location of the first occurrence:\n",
    "text.find('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated matching, use regular expressions and the `re` module.  \n",
    "To illustrate the basic mechanics of using regular expressions, suppose you want to match dates specified as digits, such as `\"11/27/2012\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = '11/27/2012'\n",
    "text2 = 'Nov 27, 2012'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple matching.\n",
    "# \\d+ means match one or more digits.\n",
    "if re.match(r'\\d+/\\d+/\\d+', text1):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if re.match(r'\\d+/\\d+/\\d+', text2):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're going to perform a lot of matches using the same pattern, it usually pays to precompile the regular expression pattern into a pattern object first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "datepat = re.compile(r'\\d+/\\d+/\\d+')\n",
    "if datepat.match(text1):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if datepat.match(text2):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `match()` method always tries to find the match at the start of a string.  \n",
    "If you want to search text for all occurrences of a pattern, use the `findall()` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10/18/2017', '5/9/2018']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Today is 10/18/2017. PyCon starts 5/9/2018.'\n",
    "datepat.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining regular expressions, it is common to introduce capture groups by enclosing parts of the pattern in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'(\\d+)/(\\d+)/(\\d+)', re.UNICODE)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)')\n",
    "datepat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture groups often simplify subsequent processing of the matched text because of the contents of each group can be extracted individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 10), match='11/27/2012'>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = datepat.match('11/27/2012')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/27/2012\n",
      "11\n",
      "27\n",
      "2012\n",
      "('11', '27', '2012')\n"
     ]
    }
   ],
   "source": [
    "# Extract the contents of each group:\n",
    "print(m.group(0))\n",
    "print(m.group(1))\n",
    "print(m.group(2))\n",
    "print(m.group(3))\n",
    "print(m.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "27\n",
      "2012\n"
     ]
    }
   ],
   "source": [
    "month, day, year = m.groups()\n",
    "print(month)\n",
    "print(day)\n",
    "print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is 10/18/2017. PyCon starts 5/9/2018.'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all matches.\n",
    "# Notice the splitting into tuples:\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', '18', '2017'), ('5', '9', '2018')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datepat.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-18\n",
      "2018-5-9\n"
     ]
    }
   ],
   "source": [
    "for month, day, year in datepat.findall(text):\n",
    "    print('{}-{}-{}'.format(year, month, day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `findall()` method searches the text and finds all matches, returning them as a list.  \n",
    "If you want to find matches iteratively, use the `finditer()` method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10', '18', '2017')\n",
      "('5', '9', '2018')\n"
     ]
    }
   ],
   "source": [
    "for m in datepat.finditer(text):\n",
    "    print(m.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic tutorial on the theory of regular expressions is beyond the scope of this book.  \n",
    "However, this recipe illustrates the absolute basics of using the `re` module to match and search for text.  \n",
    "The essential functionality is first compiling a pattern using `re.compile()` and then using methods such as `match(), findall(),` or `finditer()`.  \n",
    "When specifying patterns, it is relatively common to use raw strings such as `r'(\\d+)/(\\d+)/(\\d+)'`.  \n",
    "Such strings leave the backslash character uninterpreted, which can be useful in the context of regular expressions.  \n",
    "Otherwise, you need to use double backslashes such as `'(\\\\d+)/(\\\\d+)/(\\\\d+)'`.  \n",
    "Be aware that the `match()` method only checks the beginning of a string.  \n",
    "It’s possible that it will match things you aren’t expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 10), match='11/27/2012'>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = datepat.match('11/27/2012abcdef')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11/27/2012'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want an exact match, make sure the pattern includes the end-marker ($), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)$')\n",
    "datepat.match('11/27/2012abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 10), match='11/27/2012'>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datepat.match('11/27/2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, if you're just doing a simple text matching/searching operation, you can often skip the compilation step and use the module-level functions in the `re` module instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', '18', '2017'), ('5', '9', '2018')]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(\\d+)/(\\d+)/(\\d+)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware, though, that if you’re going to perform a lot of matching or searching, it usually pays to compile the pattern first and use it over and over again.  \n",
    "The module-level functions keep a cache of recently compiled patterns, so there isn’t a huge performance hit, but you’ll save a few lookups and extra processing by using your own compiled pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Searching and Replacing Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_searching_and_replacing_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to search for and replace a text pattern in a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple literal patterns, use the `str.replace()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'yeah, but no, but yeah, but no, but yeah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yep, but no, but yep, but no, but yep'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace('yeah', 'yep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated patterns, use the `sub()` functions/methods in the `re` module.  \n",
    "To illustrate, suppose you want to rewrite dates of the form \"11/27/2012\" as \"2012-11-27\".  \n",
    "Here's a sample of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is 2012-11-27. PyCon starts 2013-3-13.'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'\n",
    "re.sub(r'(\\d+)/(\\d+)/(\\d+)', r'\\3-\\1-\\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to `sub()` is the pattern to match and the second argument is the replacement pattern.  \n",
    "Backslashed digits such as `\\3` refer to capture group numbers in the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're going to perform repeated substitutions of the same pattern, consider compiling it first for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is 2012-11-27. PyCon starts 2013-3-13.'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)')\n",
    "datepat.sub(r'\\3-\\1-\\2', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated substitutions, it's possible to specify a substitution callback function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from calendar import month_abbr\n",
    "\n",
    "def change_date(m):\n",
    "    mon_name = month_abbr[int(m.group(1))]\n",
    "    return '{} {} {}'.format(m.group(2), mon_name, m.group(3))\n",
    "\n",
    "datepat.sub(change_date, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input, the argument to the substitution callback is a match object, as returned by `match()` or `find()`.  \n",
    "Use the `.group()` method to extract specific parts of the match.  \n",
    "The function should return the replacment text.  \n",
    "If you want to know how many substitutions were made in addition to getting the replacement text, use `re.subn()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newtext, n = datepat.subn(r'\\3-\\1-\\2', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is 2012-11-27. PyCon starts 2013-3-13.'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much more to regular expression search and replace than the `sub()` method shown.  \n",
    "The trickiest part is specifying the regular expression pattern -- something that's best left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Searching and Replacing Case-Insensitive Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_searching_and_replacing_case_insensitive_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to search for and possibly replace text in a case-insensitive manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform case-insensitive text operations, you need to use the `re` module and supply the `re.IGNORECASE` flag to various operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PYTHON', 'python', 'Python']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'UPPER PYTHON, lower python, Mixed Python'\n",
    "re.findall('python', text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UPPER snake, lower snake, Mixed snake'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('python', 'snake', text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last example reveals a limitation that replacing text won't match the case of the matched text.  \n",
    "If you need to fix this, you might have to use a support function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchcase(word):\n",
    "    def replace(m):\n",
    "        text = m.group()\n",
    "        if text.isupper():\n",
    "            return word.upper()\n",
    "        elif text.islower():\n",
    "            return word.lower()\n",
    "        elif text[0].isupper():\n",
    "            return word.capitalize()\n",
    "        else:\n",
    "            return word\n",
    "    return replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example that uses the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UPPER SNAKE, lower snake, Mixed Snake'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple cases, simply providing the `re.IGNORECASE` is enough to perform case-insensitive matching.  \n",
    "However, be aware that this may not be enough for certain kinds of Unicode matching involving case folding.  \n",
    "See [\"Working with Unicode Characters in Regular Expressions\"](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#unicodere) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Specifying a Regular Expression for the Shortest Match](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_specifying_a_regular_expression_for_the_shortest_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're trying to match a text pattern using regular expressions, but it is identifying the longest possible matches of a pattern.  \n",
    "Instead, you would like to change it to find the shortest possible match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem often arises in patterns that try to match text enclosed inside a pair of starting and ending delimiters (like a quoted string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no.']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_pat = re.compile(r'\\\"(.*)\\\"')\n",
    "text1 = 'Computer says \"no.\"'\n",
    "str_pat.findall(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no.\" Phone says \"yes.']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'Computer says \"no.\" Phone says \"yes.\"'\n",
    "str_pat.findall(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the pattern `r'\\\"(.*)\\\"'` is attempting to match text enclosed inside quotes.  \n",
    "However, the `*` operator in a regular expression is greedy, so matching is based on finding the longest possible match. Thus, in the second example involving `text2`, it incorrectly matches the two quoted strings.  \n",
    "To fix this, add the ? modifier after the `*` operator in the pattern, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no.', 'yes.']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_pat = re.compile(r'\\\"(.*?)\\\"')\n",
    "str_pat.findall(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the matching nongreedy, and produces the shortest match instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe addresses one of the more common problems encountered when writing regular expressions involving the dot (.) character.  \n",
    "In a pattern, the dot matches any character except a newline.  \n",
    "However, if you bracket the dot with starting and ending text (such as a quote), matching will try to find the longest possible match to the pattern.  \n",
    "This causes multiple occurrences of the starting or ending text to be skipped altogether and included in the results of the longer match.  \n",
    "Adding the ? right after operators such as `*` or `+` forces the matching algorithm to look for the shortest possible match instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Writing a Regular Expression for Multiline Patterns](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_writing_a_regular_expression_for_multiline_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're trying to match a block of text using a regular expression, but you need the match to span multiple lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem typically arises in patterns that use the dot(.) to match any character but forget to account for the fact that it doesn't match newlines.  \n",
    "For example, suppose you are trying to match C-style comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' this is a comment ']"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = re.compile(r'/\\*(.*?)\\*/')\n",
    "text1 = '/* this is a comment */'\n",
    "comment.findall(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = '''/* this is a \n",
    "              multiline comment */\n",
    "'''\n",
    "comment.findall(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the problem, you can add support for newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' this is a \\n              multiline comment ']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = re.compile(r'/\\*((?:.|\\n)*?)\\*/')\n",
    "comment.findall(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pattern, `(?:.|\\n)` specifies a noncapture group, which basically means that it defines a group for the purposes of matching, but that group is not captured separately or numbered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `re.compile()` function accepts a flag, `re.DOTALL`, which is useful here.  \n",
    "It makes the . in a regular expression match all characters, including newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' this is a \\n              multiline comment ']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = re.compile(r'/\\*(.*?)\\*/', re.DOTALL)\n",
    "comment.findall(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `re.DOTALL` flag works fine for simple cases, but might be problematic if you’re working with extremely complicated patterns or a mix of separate regular expressions that have been combined together for the purpose of tokenizing, as described in [\"Tokenizing Text\"](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#tokenizing).  \n",
    "If given a choice, it’s usually better to define your regular expression pattern so that it works correctly without the need for extra flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Normalizing Unicode Text to a Standard Representation](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#normalizingunicodetext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're working with Unicode strings, but you need to make sure that all of the strings have the same underlying representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Unicode, certain characters can be represented by more than one valid sequence of code points.  \n",
    "To illustrate, consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = 'Spicy Jalape\\u00f1o'\n",
    "s2 = 'Spicy Jalapen\\u0303o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(s1))\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(s2))\n",
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the text \"Spicy Jalapeño\" has been presented in two forms.  \n",
    "The first uses the fully composed \"ñ\" character (U+00F1).  \n",
    "The second uses the Latin letter \"n\" followed by a \"~\" combining character (U+0303)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having multiple representations is a problem for programs that compare strings.  \n",
    "In order to fix this, you should first normalize the text into a standard representation using the [unicodedata module](https://docs.python.org/3/library/unicodedata.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "t1 = unicodedata.normalize('NFC', s1)\n",
    "t2 = unicodedata.normalize('NFC', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Spicy Jalape\\xf1o'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ascii(t1))\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Spicy Jalape\\xf1o'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ascii(t2))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3 = unicodedata.normalize('NFD', s1)\n",
    "t4 = unicodedata.normalize('NFD', s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 == t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "'Spicy Jalapen\\u0303o'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(t3))\n",
    "print(ascii(t3))\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "'Spicy Jalapen\\u0303o'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeño'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(t4))\n",
    "print(ascii(t4))\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to `normalize()` specifies how you want the string normalized.  \n",
    "`NFC` means that characters should be fully composed (i.e., use a single code point if possible).  \n",
    "`NFD` means that characters should be fully decomposed with the use of combining characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also supports the normalization forms [NFKC and NFKD](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize), which add extra compatibility features for dealing with certain kinds of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﬁ'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A single character:\n",
    "s = '\\ufb01'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﬁ'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFD', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "# Notice how the combined letters are broken apart here:\n",
    "print(unicodedata.normalize('NFKD', s))\n",
    "print(unicodedata.normalize('NFKC', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is an important part of any code that needs to ensure that it processes Unicode text in a sane and consistent way.  \n",
    "This is especially true when processing strings received as part of user input where you have little control over the encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization can also be an important part of sanitizing and filtering text.  \n",
    "For example, suppose you want to remove all diacritical marks from some text (possibly for the purposes of searching or matching):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spicy Jalapeno'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = unicodedata.normalize('NFD', s1)\n",
    "''.join(c for c in t1 if not unicodedata.combining(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last example shows another important aspect of the `unicodedata module` -- namely, utility functions for testing characters against character classes.  \n",
    "The `combining()` function tests a character to see if it is a combining character.  \n",
    "There are other functions in the module for finding character categories, testing digits, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode is obviously a large topic.  \n",
    "For more detailed reference information about normalization, visit [Unicode’s page on the subject](https://www.unicode.org/faq/normalization.html).  \n",
    "Ned Batchelder has also given an excellent presentation on Python Unicode handling issues [on his website](https://nedbatchelder.com/text/unipain.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Working with Unicode Characters in Regular Expressions](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#unicodere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are using regular expressions to process text, but are concerned about the handling of Unicode characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `re` module is already programmed with rudimentary knowledge of certain Unicode character classes.  \n",
    "For example, `\\d` already matches any unicode digit character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\d+', re.UNICODE)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num = re.compile('\\d+')\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='123'>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASCII digits:\n",
    "num.match('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='١٢٣'>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arabic digits:\n",
    "num.match('\\u0661\\u0662\\u0663')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to include specific Unicode characters in patterns, you can use the usual escape sequence for Unicode characters (e.g., `\\uFFFF` or `\\UFFFFFFF`).  \n",
    "For example, here is a regex that matches all characters in a few different Arabic code pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[\\u0600-ۿݐ-ݿࢠ-ࣿ]+', re.UNICODE)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic = re.compile('[\\u0600-\\u06ff\\u0750-\\u077f\\u08a0-\\u08ff]+')\n",
    "arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing matching and searching operations, it’s a good idea to normalize and possibly sanitize all text to a standard form first (see [\"Normalizing Unicode Text to a Standard Representation\"](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#normalizingunicodetext)).  \n",
    "However, it’s also important to be aware of special cases.  \n",
    "For example, consider the behavior of case-insensitive matching combined with case folding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'straße', re.IGNORECASE|re.UNICODE)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat = re.compile('stra\\u00dfe', re.IGNORECASE)\n",
    "pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 6), match='straße'>\n"
     ]
    }
   ],
   "source": [
    "s = 'straße'\n",
    "print(pat.match(s))  # Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pat.match(s.upper()))  # Doesn't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STRASSE'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upper()  # Case folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing Unicode and regular expressions is often a good way to make your head explode.  \n",
    "If you’re going to do it seriously, you should consider installing the [third-party regex library](https://pypi.python.org/pypi/regex), which provides full support for Unicode case folding, as well as a variety of other interesting features, including approximate matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Stripping Unwanted Characters from Strings](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_stripping_unwanted_characters_from_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to strip unwanted characters, such as whitespace, from the beginning, end, or middle of a text string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `strip()` method can be used to strip characters from the beginning or end of a string.  \n",
    "The `lstrip()` and `rstrip()` methods perform stripping from the left or right side.  \n",
    "By default, these methods strip whitespace, but other characters can be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    hello world \\n'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whitespace stripping:\n",
    "s = '    hello world \\n'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world \\n'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    hello world'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-----hello====='"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character stripping:\n",
    "t = '-----hello====='\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-----hello====='"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello====='"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lstrip('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello====='"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.strip('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.strip('-=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various `strip()` methods are commonly used when reading and cleaning up data for later processing.  \n",
    "For example, you can use them to get rid of whitespace, remove quotations, and other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that stripping does not apply to any text in the midle of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  hello       world   \\n'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '  hello       world   \\n'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello       world'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = s.strip()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to do something to the inner space, you need to use another technique, such as using the `replace()` method or a regular expression substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helloworld'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub('\\s+', ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that you want to combine string stripping operations with some other kind of iterative processing, such as reading lines of data from a file.  \n",
    "If so, this is one area where a generator expression can be useful:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(filename) as f:\n",
    "    lines = (line.strip() for line in f)\n",
    "    for line in lines:\n",
    "        ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the expression `lines = (line.strip() for line in f)` acts as a kind of data transform.  \n",
    "It’s efficient because it doesn’t actually read the data into any kind of temporary list first.  \n",
    "It just creates an iterator where all of the lines produced have the stripping operation applied to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more advanced stripping, you might turn to the `translate()` method.  \n",
    "See the next recipe on sanitizing strings for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sanitizing and Cleaning Up Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_sanitizing_and_cleaning_up_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some bored script kiddie has entered the text \"pýtĥöñ\" into a form on your web page and you’d like to clean it up somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of sanitizing and cleaning up text applies to a wide variety of problems involving text parsing and data handling.  \n",
    "At a very simple level, you might use basic string functions (e.g., `str.upper()` and `str.lower()`) to convert text to a standard case.  \n",
    "Simple replacements using `str.replace()` or `re.sub()` can focus on removing or changing very specific character sequences.  \n",
    "You can also normalize text using `unicodedata.normalize()`, as shown in [\"Normalizing Unicode Text to a Standard Representation\"](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#normalizingunicodetext)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might want to take the sanitation process a step further.  \n",
    "Perhaps, for example, you want to eliminate whole ranges of characters or strip diacritical marks.  \n",
    "To do so, you can turn to the often overlooked `str.translate()` method.  \n",
    "To illustrate, suppose you’ve got a messy string such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pýtĥöñ\\x0cis\\tawesome\\r\\n'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'pýtĥöñ\\fis\\tawesome\\r\\n'\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to clean up the whitespace.  \n",
    "To do this, make a small translation table and use `translate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remap = {\n",
    "    ord('\\t') : ' ',\n",
    "    ord('\\f') : ' ',\n",
    "    ord('\\r') : None  # Deleted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pýtĥöñ is awesome\\n'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = s.translate(remap)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, whitespace characters such as `\\t` and `\\f` have been remapped to a single space.  \n",
    "The carriage return `\\r` has been deleted entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take this remapping idea a step further and build much bigger tables.  \n",
    "For example, let’s remove all combining characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(768, None), (769, None), (770, None), (771, None), (772, None)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode)\n",
    "                         if unicodedata.combining(chr(c)))\n",
    "# Show the first five entries in the table:\n",
    "list(itertools.islice(cmb_chrs.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pýtĥöñ is awesome\\n'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = unicodedata.normalize('NFD', a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python is awesome\\n'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.translate(cmb_chrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, a dictionary mapping every Unicode combining character to `None` is created using the `dict.fromkeys()`.  \n",
    "The original input is then normalized into a decomposed form using `unicodedata.normalize()`.  \n",
    "From there, the translate function is used to delete all of the accents.  \n",
    "Similar techniques can be used to remove other kinds of characters (e.g., control characters, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, here is a translation table that maps all Unicode decimal digit characters to their equivalent in ASCII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitmap = { c: ord('0') + unicodedata.digit(chr(c))\n",
    "            for c in range(sys.maxunicode)\n",
    "            if unicodedata.category(chr(c)) == 'Nd' }\n",
    "len(digitmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, 48), (49, 49), (50, 50), (51, 51), (52, 52)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(digitmap.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arabic digits:\n",
    "x = '\\u0661\\u0662\\u0663'\n",
    "x.translate(digitmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another technique for cleaning up text involves I/O decoding and encoding functions.  \n",
    "The idea here is to first do some preliminary cleanup of the text, and then run it through a combination of `encode()` or `decode()` operations to strip or alter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pýtĥöñ is awesome\\n'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python is awesome\\n'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = unicodedata.normalize('NFD', a)\n",
    "b.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the normalization process decomposed the original text into characters along with separate combining characters.  \n",
    "The subsequent ASCII encoding/decoding simply discarded all of those characters in one fell swoop.  \n",
    "Naturally, this would only work if getting an ASCII representation was the final goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major issue with sanitizing text can be runtime performance.  \n",
    "As a general rule, the simpler it is, the faster it will run.  \n",
    "For simple replacements, the `str.replace()` method is often the fastest approach -- even if you have to call it multiple times.  \n",
    "For instance, to clean up whitespace, you could use code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_spaces(s):\n",
    "    s = s.replace('\\r', '')\n",
    "    s = s.replace('\\t', ' ')\n",
    "    s = s.replace('\\f', ' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try it, you’ll find that it’s quite a bit faster than using `translate()` or an approach using a regular expression.  \n",
    "On the other hand, the `translate()` method is very fast if you need to perform any kind of nontrivial character-to-character remapping or deletion.  \n",
    "In the big picture, performance is something you will have to study further in your particular application.  \n",
    "Unfortunately, it’s impossible to suggest one specific technique that works best for all cases, so try different approaches and measure it.  \n",
    "Although the focus of this recipe has been text, similar techniques can be applied to bytes, including simple replacements, translation, and regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Aligning Text Strings](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_aligning_text_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to format text with some sort of alignment applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For basic alignment of strings, the `ljust()`, `jrust()`, and `center()` methods of strings can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World         '"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hello World'\n",
    "text.ljust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         Hello World'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.rjust(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Hello World     '"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.center(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these methods accept an optional &&65.180&&fill character as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=========Hello World'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.rjust(20,'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'****Hello World*****'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.center(20,'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `format()` function can also be used to easily align things.  \n",
    "All you need to do is use the <, >, or ^ characters along with a desired width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         Hello World'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(text, '>20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World         '"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(text, '<20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Hello World     '"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(text, '^20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to include a fill character other than a space, specify it before the alignment character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=========Hello World'"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(text, '=>20s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'****Hello World*****'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(text, '*^20s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These format codes can also be used in the `format()` method when formatting multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     Hello      World'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:>10s} {:>10s}'.format('Hello', 'World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of `format()` is that it is not specific to strings.  \n",
    "It works with any value, making it more general purpose.  \n",
    "For instance, you can use it with numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    1.2345'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1.2345\n",
    "format(x, '>10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   1.23   '"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(x, '^10.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In older code, you will also see the % operator used to format text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World         '"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%-20s' % text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         Hello World'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%20s' % text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in new code, you should probably prefer the use of the `format()` function or method.  \n",
    "The `format()` method is a lot more powerful than what is provided with the % operator.  \n",
    "Moreover, `format()` is more general purpose than using the `jlust()`, `rjust()`, or `center()` method of strings in that it works with any kind of object.  \n",
    "For a complete list of features available with the `format()` function, consult the [online Python documentation](https://docs.python.org/3/library/string.html#formatspec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Combining and Concatenating Strings](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_combining_and_concatenating_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to combine many small strings together into a larger string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the strings you wish to combine are found in a sequence or iterable, the fastest way to combine them is to use the `join()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Chicago Not Chicago?'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = ['Is', 'Chicago', 'Not', 'Chicago?']\n",
    "' '.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is,Chicago,Not,Chicago?'"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IsChicagoNotChicago?'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this syntax might look really odd, but the `join()` operation is specified as a method on strings.  \n",
    "Partly this is because the objects you want to join could come from any number of different data sequences (e.g., lists, tuples, dicts, files, sets, or generators), and it would be redundant to have `join()` implemented as a method on all of those objects separately.  \n",
    "So you just specify the separator string that you want and use the `join()` method on it to glue text fragments together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're only combining a few strings, using the `+` operator usually works well enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Chicago Not Chicago?'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Is Chicago'\n",
    "b = 'Not Chicago?'\n",
    "a + ' ' + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `+` operator also works fine as a substitute for more complicated string formatting operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Chicago Not Chicago?\n"
     ]
    }
   ],
   "source": [
    "print('{} {}'.format(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Chicago Not Chicago?\n"
     ]
    }
   ],
   "source": [
    "print(a + ' ' + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're trying to combine string literals together in source code, you can simply place them adjacent to each other with no `+` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HelloWorld'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Hello' 'World'\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining strings together might not seem advanced enough to warrant an entire recipe, but it’s often an area where programmers make programming choices that severely impact the performance of their code.  \n",
    "The most important thing to know is that using the + operator to join a lot of strings together is grossly inefficient due to the memory copies and garbage collection that occurs.  \n",
    "In particular, you never want to write code that joins strings together like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = ''\n",
    "for p in parts:\n",
    "    s += p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs quite a bit slower than using the `join()` method, mainly because each `+=` operation creates a new string object.  \n",
    "You’re better off just collecting all of the parts first and then joining them together at the end.  \n",
    "One related (and pretty neat) trick is the conversion of data to strings and concatenation at the same time using a generator expression, as described in [\"Transforming and Reducing Data at the Same Time\"](http://chimera.labs.oreilly.com/books/1230000000393/ch01.html#generatorargs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACME,50,91.1'"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['ACME', 50, 91.1]\n",
    "','.join(str(d) for d in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, be on the lookout for unnecessary string concatenations.  \n",
    "Sometimes programmers get carried away with concatenation when it's really not technically necessary.  \n",
    "For example, when printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One:Two:Three\n"
     ]
    }
   ],
   "source": [
    "a = 'One'\n",
    "b = 'Two'\n",
    "c = 'Three'\n",
    "# Ugly\n",
    "print(a + ':' + b + ':' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One:Two:Three\n"
     ]
    }
   ],
   "source": [
    "# Still ugly:\n",
    "print(':'.join([a, b, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One:Two:Three\n"
     ]
    }
   ],
   "source": [
    "# Better:\n",
    "print(a, b, c, sep=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing I/O operations and string concatenation is something that might require study in your application.  \n",
    "For example, consider the following two code fragments:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Version 1 -- string concatentation:\n",
    "f.write(chunk1 + chunk2)\n",
    "\n",
    "# Version 2 -- separate I/O operations:\n",
    "f.write(chunk1)\n",
    "f.write(chunk2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the two strings are small, the first version might offer much better performance due to the inherent expense of carrying out an I/O system call.  \n",
    "On the other hand, if the two strings are large, the second version may be more efficient, since it avoids making a large temporary result and copying large blocks of memory around.  \n",
    "Again, it must be stressed that this is something you would have to study in relation to your own data in order to determine which performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you’re writing code that is building output from lots of small strings, you might consider writing that code as a generator function, using `yield` to emit fragments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    yield 'Is'\n",
    "    yield 'Chicago'\n",
    "    yield 'Not'\n",
    "    yield 'Chicago?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'Chicago', 'Not', 'Chicago?']\n"
     ]
    }
   ],
   "source": [
    "print(list(sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing about this approach is that it makes no assumptions about how the fragments are to be assembled together.  \n",
    "For example, you could simply join the fragments using `join()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Chicago Not Chicago?'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(sample())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could redirect the fragments to I/O:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for part in sample():\n",
    "    f.write(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could come up with some kind of hybrid scheme that's smart about combining I/O operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(source, maxsize):\n",
    "    parts = []\n",
    "    size = 0\n",
    "    for part in source:\n",
    "        parts.append(part)\n",
    "        size += len(part)\n",
    "        if size > maxsize:\n",
    "            yield ''.join(parts)\n",
    "            parts = []\n",
    "            size = 0\n",
    "    yield ' '.join(parts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for part in combine(sample(), 32768):\n",
    "    f.write(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point is that the original generator function doesn't have to know the precise details; it just yields the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Interpolating Variables in Strings](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to create a string in which embedded variable names are substituted with a string representation of a variable's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has no direct support for simply substituting variable values in strings.  \n",
    "However, this feature can be approximated using the `format()` method of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido has 37 messages.'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '{name} has {n} messages.'\n",
    "s.format(name='Guido', n=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively,if the values to be substituted are truly found in variables, you can use the combination of `format_map()` and `vars()`, as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido has 37 messages.'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Guido'\n",
    "n = 37\n",
    "s.format_map(vars())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One subtle feature of `vars()` is that it also works with instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Info:\n",
    "    def __init__(self, name, n):\n",
    "        self.name = name\n",
    "        self.n = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido has 37 messages.'"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Info('Guido', 37)\n",
    "s.format_map(vars(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One downside of `format()` and `format_map()` is that they do not deal gracefully with missing values:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s.format(name='Guido')\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "<ipython-input-488-dac1742a40f0> in <module>()\n",
    "----> 1 s.format(name='Guido')\n",
    "\n",
    "KeyError: 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to avoid this is to define an alternative dictionary class with a `__missing__()` method, as in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class safesub(dict):\n",
    "    def __missing__(self, key):\n",
    "        return '{' + key + '}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this class to wrap the inputs to `format_map()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido has {n} messages.'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that n is undefined:\n",
    "del n\n",
    "s.format_map(safesub(vars()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find yourself frequently performing these steps in your code, you could hide the variable substitution process behind a small utility function that employs a so-called [\"frame hack\"](http://farmdev.com/src/secrets/framehack/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def sub(text):\n",
    "    return text.format_map(safesub(sys._getframe(1).f_locals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'Guido'\n",
    "n = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Guido\n"
     ]
    }
   ],
   "source": [
    "print(sub('Hello {name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 37 messages.\n"
     ]
    }
   ],
   "source": [
    "print(sub('You have {n} messages.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your favorite color is {color}.\n"
     ]
    }
   ],
   "source": [
    "print(sub('Your favorite color is {color}.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of true variable interpolation in Python has led to a variety of solutions over the years.  \n",
    "As an alternative to the solution presented in this recipe, you will sometimes see string formatting like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = 'Guido'\n",
    "n = 37\n",
    "'%(name) has %(n) messages.' % vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also see the use of template strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guido has 37 messages.'"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "s = string.Template('$name has $n messages.')\n",
    "s.substitute(vars())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `format()` and `format_map()` methods are more modern than either of these alternatives, and should be preferred.  \n",
    "One benefit of using `format()` is that you also get all of the features related to string formatting (alignment, padding, numerical formatting, etc.), which is simply not possible with alternatives such as `Template` string objects.  \n",
    "Parts of this recipe also illustrate a few interesting advanced features.  \n",
    "The little-known `__missing__()` method of mapping/dict classes is a method that you can define to handle missing values.  \n",
    "In the `safesub` class, this method has been defined to return missing values back as a placeholder.  \n",
    "Instead of getting a `KeyError` exception, you would see the missing values appearing in the resulting string (potentially useful for debugging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub()` function uses `sys._getframe(1)` to return the stack frame of the caller.  \n",
    "From that, the `f_locals` attribute is accessed to get the local variables.  \n",
    "It goes without saying that messing around with stack frames should probably be avoided in most code.  \n",
    "However, for utility functions such as a string substitution feature, it can be useful.  \n",
    "As an aside, it’s probably worth noting that `f_locals` is a dictionary that is a copy of the local variables in the calling function.  \n",
    "Although you can modify the contents of `f_locals`, the modifications don’t actually have any lasting effect.  \n",
    "Thus, even though accessing a different stack frame might look evil, it’s not possible to accidentally overwrite variables or change the local environment of the caller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Reformatting Text to a Fixed Number of Columns](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_reformatting_text_to_a_fixed_number_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have long strings that you want to reformat so that they fill a user-specified number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `textwrap` module to reformat text for output.  \n",
    "For example, suppose you have the following long string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under.\""
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Look into my eyes, look into my eyes, the eyes, the eyes, \\\n",
    "the eyes, not around the eyes, don't look around the eyes, \\\n",
    "look into my eyes, you're under.\"\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can use the `textwrap` module to reformat it in various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look into my eyes, look into my eyes, the eyes, the eyes, the eyes,\n",
      "not around the eyes, don't look around the eyes, look into my eyes,\n",
      "you're under.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(s, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look into my eyes, look into my eyes,\n",
      "the eyes, the eyes, the eyes, not around\n",
      "the eyes, don't look around the eyes,\n",
      "look into my eyes, you're under.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(s, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Look into my eyes, look into my\n",
      "eyes, the eyes, the eyes, the eyes, not\n",
      "around the eyes, don't look around the\n",
      "eyes, look into my eyes, you're under.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(s, 40, initial_indent='    '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look into my eyes, look into my eyes,\n",
      "    the eyes, the eyes, the eyes, not\n",
      "    around the eyes, don't look around\n",
      "    the eyes, look into my eyes, you're\n",
      "    under.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(s, 40, subsequent_indent='    '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `textwrap` module is a straightforward way to clean up text for printing -- especially if you want the output to fit nicely on the terminal.  \n",
    "On the subject of the terminal size, you can obtain it using `os.get_terminal_size()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.get_terminal_size().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fill()` method has a few additional options that control how it handles tabs, sentence endings, and so on.  \n",
    "Look at the [documentation for the textwrap.TextWrapper class](https://docs.python.org/3.3/library/textwrap.html#textwrap.TextWrapper) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Handling HTML and XML Entities in Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_handling_html_and_xml_entities_in_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to replace HTML or XML entities such as `&entity;` or `&#code;` with their corresponding text.  \n",
    "Alternatively, you need to produce text, but escape certain characters (e.g., <, >, or &)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are producing text, replacing special characters such as < or > is relativley easy if you use the `html.escape()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 'Elements are written as \"<tag>text</tag>\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements are written as \"<tag>text</tag>\".\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements are written as &quot;&lt;tag&gt;text&lt;/tag&gt;&quot;.\n"
     ]
    }
   ],
   "source": [
    "print(html.escape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements are written as \"&lt;tag&gt;text&lt;/tag&gt;\".\n"
     ]
    }
   ],
   "source": [
    "# Disable escaping of quotes:\n",
    "print(html.escape(s, quote=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're trying to emit text as ASCII and want to embed character code entities for non-ASCII characters, you can use the `errors='xmlcharrefreplace'` argument to various I/O-related functions to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Spicy Jalape&#241;o'"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Spicy Jalapeño'\n",
    "s.encode('ascii', errors='xmlcharrefreplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace entities in text, a different approach is needed.  \n",
    "If you’re actually processing HTML or XML, try using a proper HTML or XML parser first.  \n",
    "Normally, these tools will automatically take care of replacing the values for you during parsing and you don’t need to worry about it.  \n",
    "If, for some reason, you’ve received bare text with some entities in it and you want them replaced manually, you can usually do it using various utility functions/methods associated with HTML or XML parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spicy &quot;Jalape&#241;o&quot.'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Spicy &quot;Jalape&#241;o&quot.'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamingrove/.pyenv/versions/3.6.1/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Spicy \"Jalapeño\".'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "p = HTMLParser()\n",
    "p.unescape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The prompt is &gt;&gt;&gt;'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'The prompt is &gt;&gt;&gt;'\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The prompt is >>>'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xml.sax.saxutils import unescape\n",
    "unescape(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper escaping of special characters is an easily overlooked detail of generating HTML or XML.  \n",
    "This is especially true if you’re generating such output yourself using `print()` or other basic string formatting features.  \n",
    "Using a utility function such as `html.escape()` is an easy solution.  \n",
    "If you need to process text in the other direction, various utility functions, such as `xml.sax.saxutils.unescape()`, can help.  \n",
    "However, you really need to investigate the use of a proper parser.  \n",
    "For example, if processing HTML or XML, using a parsing module such as `html.parser` or `xml.etree.ElementTree` should already take care of details related to replacing entities in the input text for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tokenizing Text](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a string that you want to parse left to right into a stream of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a sstring of text such as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'foo = 23 + 42 * 10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize the string, you need to do more than merely match patterns.  \n",
    "You need to have some way to identify the kind of pattern as well.  \n",
    "For instance, you might want to turn the string into a sequence of pairs like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NAME', 'foo'),\n",
       " ('EQ', '='),\n",
       " ('NUM', '23'),\n",
       " ('PLUS', '+'),\n",
       " ('NUM', '42'),\n",
       " ('TIMES', '*'),\n",
       " ('NUM', '10')]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [('NAME', 'foo'), ('EQ', '='), ('NUM', '23'), ('PLUS', '+'),\n",
    "          ('NUM', '42'), ('TIMES', '*'), ('NUM', '10')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this kind of splitting, the first step is to define all of the possible tokens, including whitespace, by regular expression patterns using named capture groups such as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)|(?P<NUM>\\d+)|(P?<PLUS>\\+)|(?P<TIMES>\\*)|(?P<EQ>=)|(?P<WS>\\s+)',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "NAME  = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\n",
    "NUM   = r'(?P<NUM>\\d+)'\n",
    "PLUS  = r'(P?<PLUS>\\+)'\n",
    "TIMES = r'(?P<TIMES>\\*)'\n",
    "EQ    = r'(?P<EQ>=)'\n",
    "WS    = r'(?P<WS>\\s+)'\n",
    "\n",
    "master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))\n",
    "master_pat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these `re` patterns, the `?P<TOKENNAME>` convention is used to assign a name to the pattern.  \n",
    "This will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to tokenize, use the little-known `scanner()` method of pattern objects.  \n",
    "This method creates a scanner object in which repeated calls to `match()` step through the supplied text one match at a time.  \n",
    "Here is an interactive example of how a scanner object works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scanner = master_pat.scanner('foo = 42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='foo'>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NAME', 'foo')"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.lastgroup, _.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(3, 4), match=' '>"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WS', ' ')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.lastgroup, _.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(4, 5), match='='>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EQ', '=')"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.lastgroup, _.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(5, 6), match=' '>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WS', ' ')"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.lastgroup, _.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(6, 8), match='42'>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NUM', '42')"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.lastgroup, _.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take this technique and put it into code, it can be cleaned up and easily packaged into a generator like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Token = namedtuple('Token', ['type', 'value'])\n",
    "\n",
    "def generate_tokens(pat, text):\n",
    "    scanner = pat.scanner(text)\n",
    "    for m in iter(scanner.match, None):\n",
    "        yield Token(m.lastgroup, m.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NAME', value='foo')\n",
      "Token(type='WS', value=' ')\n",
      "Token(type='EQ', value='=')\n",
      "Token(type='WS', value=' ')\n",
      "Token(type='NUM', value='42')\n"
     ]
    }
   ],
   "source": [
    "# Example use:\n",
    "for tok in generate_tokens(master_pat, 'foo = 42'):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to filter the token stream in some way, you can either define more generator functions or use a generator expression.  \n",
    "For example, here is how you might filter out all whitespace tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NAME', value='foo')\n",
      "Token(type='EQ', value='=')\n",
      "Token(type='NUM', value='23')\n"
     ]
    }
   ],
   "source": [
    "tokens = (tok for tok in generate_tokens(master_pat, text)\n",
    "          if tok.type != 'WS')\n",
    "for tok in tokens:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing is often the first step for more advanced kinds of text parsing and handling.  \n",
    "To use the scanning technique shown, there are a few important details to keep in mind.  \n",
    "First, you must make sure that you identify every possible text sequence that might appear in the input with a correponding `re` pattern.  \n",
    "If any nonmatching text is found, scanning simply stops.  \n",
    "This is why it was necessary to specify the whitespace (WS) token in the example.  \n",
    "The order of tokens in the master regular expression also matters.  \n",
    "When matching, `re` tries to match pattens in the order specified.  \n",
    "Thus, if a pattern happens to be a substring of a longer pattern, you need to make sure the longer pattern goes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LT = r'(?P<LT><)'\n",
    "LE = r'(?P<LE><)'\n",
    "EQ = r'(?P<EQ>=)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_pat = re.compile('|'.join([LE, LT, EQ]))   ## Correct\n",
    "# master_pat = re.compile('|'.join([LT, LE, EQ])) ## Incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second pattern is wrong because it would match the text <= as the token LT followed by the token EQ, not the single token LE, as was probably desired.  \n",
    "Also, watch out for patterns that form substrings.  \n",
    "For example, suppose you have two patterns like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRINT = r'(P<PRINT>print)'\n",
    "NAME  = r'(P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\n",
    "\n",
    "master_pat = re.compile('|'.join([PRINT, NAME]))\n",
    "\n",
    "for tok in generate_tokens(master_pat, 'printer'):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced kinds of tokenizing, you may want to check out packages such as [PyParsing](http://pyparsing.wikispaces.com/) or [PLY (Python Lex-Yacc)](http://www.dabeaz.com/ply/index.html).  \n",
    "An example involving PLY appears in the next recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Writing a Simple Recursive Descent Parser](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#_writing_a_simple_recursive_descent_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to [parse text according to a set of grammar rules](https://en.wikipedia.org/wiki/Recursive_descent_parser) and perform actions or build an [abstract syntax tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree) representing the input.  \n",
    "The grammar is small, so you would prefer to just write the parser yourself as opposed to using some kind of framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we're focused on the problem of parsing text according to a [particular grammar](https://en.wikipedia.org/wiki/Syntax_%28programming_languages%29).  \n",
    "In order to do this, you should probably start by having a [formal specification of the grammar](https://docs.python.org/3/reference/grammar.html) in the form of a [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form) or [EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form).  \n",
    "For example, a grammar for simple arithmetic expressions might look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "expr ::= expr + term\n",
    "     |   expr - term\n",
    "     |   term\n",
    "        \n",
    "term ::= term * factor\n",
    "     |   term / factor\n",
    "     |   factor\n",
    "        \n",
    "factor ::= ( expr )\n",
    "       |   NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternatively, in EBNF form:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "expr ::= term { (+|-) term }*\n",
    "\n",
    "term ::= factor { (*|/) factor }*\n",
    "\n",
    "factor ::= ( expr )\n",
    "       |   NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In an EBNF, parts of a rule enclosed in `{ ... }*` are optional.  \n",
    "The `*` means zero or more repetitions (the same meaning as in a regular expression).  \n",
    "Now, if you’re not familiar with the mechanics of working with a BNF, think of it as a specification of substitution or replacement rules where symbols on the left side can be replaced by the symbols on the right (or vice versa).  \n",
    "Generally, what happens during parsing is that you try to match the input text to the grammar by making various substitutions and expansions using the BNF.  \n",
    "To illustrate, suppose you are parsing an expression such as 3 + 4 * 5.  \n",
    "This expression would first need to be broken down into a token stream, using the techniques described in [\"Tokenizing Text\"](http://chimera.labs.oreilly.com/books/1230000000393/ch02.html#tokenizing).  \n",
    "The result might be a sequence of tokens like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NUM * NUM * NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, parsing involves trying to match the grammar to input tokens by making substitutions:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "expr\n",
    "expr ::= term { (+|-) term }*\n",
    "expr ::= factor { (*|/) factor }* { (+|-) term }*\n",
    "expr ::= NUM { (*|/) factor }* { (+|-) term }*\n",
    "expr ::= NUM { (+|-) term }*\n",
    "expr ::= NUM + term { (+|-) term }*\n",
    "expr ::= NUM + factor { (*|/) factor }* { (+|-) term }*\n",
    "expr ::= NUM + NUM { (*|/) factor}* { (+|-) term }*\n",
    "expr ::= NUM + NUM * factor { (*|/) factor }* { (+|-) term }*\n",
    "expr ::= NUM + NUM * NUM { (*|/) factor }* { (+|-) term }*\n",
    "expr ::= NUM + NUM * NUM { (+|-) term }*\n",
    "expr ::= NUM + NUM * NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following all of the substitution steps takes a bit of coffee, but they’re driven by looking at the input and trying to match it to grammar rules.  \n",
    "The first input token is a `NUM`, so substitutions first focus on matching that part.  \n",
    "Once matched, attention moves to the next token of `+` and so on.  \n",
    "Certain parts of the righthand side (e.g., `{ (*/) factor }*`) disappear when it’s determined that they can’t match the next token.  \n",
    "In a successful parse, the entire righthand side is expanded completely to match the input token stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of the preceding background in place, here is a simple recipe that shows how to build a recursive descent expression evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "# Token specification:\n",
    "NUM    = r'(?P<NUM>\\d+)'\n",
    "PLUS   = r'(?P<PLUS>\\+)'\n",
    "MINUS  = r'(?P<MINUS>-)'\n",
    "TIMES  = r'(?P<TIMES>\\*)'\n",
    "DIVIDE = r'(?P<DIVIDE>/)'\n",
    "LPAREN = r'(?P<LPAREN>\\()'\n",
    "RPAREN = r'(?P<RPAREN>\\))'\n",
    "WS     = r'(?P<WS>\\s+)'\n",
    "\n",
    "master_pat = re.compile('|'.join([NUM, PLUS, MINUS, TIMES,\n",
    "                                  DIVIDE, LPAREN, RPAREN, WS]))\n",
    "\n",
    "# Tokenizer\n",
    "Token = collections.namedtuple('Token', ['type', 'value'])\n",
    "\n",
    "def generate_tokens(text):\n",
    "    scanner = master_pat.scanner(text)\n",
    "    for m in iter(scanner.match, None):\n",
    "        tok = Token(m.lastgroup, m.group())\n",
    "        if tok.type != 'WS':\n",
    "            yield tok\n",
    "            \n",
    "# Parser:\n",
    "class ExpressionEvaluator:\n",
    "    \"\"\"\n",
    "    Implementation of a recursive descent parser.\n",
    "    Each method implements a single grammar rule.\n",
    "    Use the ._accept() method to test and accept the current lookahead token.\n",
    "    Use the ._expect() method to exactly match and discard the next token on the input,\n",
    "    or raise a SyntaxError if it doesn't match.\n",
    "    \"\"\"\n",
    "    \n",
    "    def parse(self, text):\n",
    "        self.tokens = generate_tokens(text)\n",
    "        self.tok = None      # Last symbol consumed\n",
    "        self.nexttok = None  # Next symbol tokenized\n",
    "        self._advance()      # Load first lookahead token\n",
    "        return self.expr()\n",
    "    \n",
    "    def _advance(self):\n",
    "        'Advance one token ahead'\n",
    "        self.tok, self.nexttok = self.nexttok, next(self.tokens, None)\n",
    "        \n",
    "    def _accept(self, toktype):\n",
    "        'Test and consume the next token if it matches toktype'\n",
    "        if self.nexttok and self.nexttok.type == toktype:\n",
    "            self._advance()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def _expect(self, toktype):\n",
    "        'Consume next token if if matches toktype or raise SyntaxError'\n",
    "        if not self._accept(toktype):\n",
    "            raise SyntaxError('Expected ' + toktype)\n",
    "            \n",
    "    # Grammar rules:\n",
    "    \n",
    "    def expr(self):\n",
    "        \"expression ::= term { ('+'|'-') term }*\"\n",
    "        \n",
    "        exprval = self.term()\n",
    "        while self._accept('PLUS') or self._accept('MINUS'):\n",
    "            op = self.tok.type\n",
    "            right = self.term()\n",
    "            if op == 'PLUS':\n",
    "                exprval += right\n",
    "            elif op == 'MINUS':\n",
    "                exprval -= right\n",
    "        return exprval\n",
    "    \n",
    "    def term(self):\n",
    "        \"term ::= factor { ('*'|'/') factor }*\"\n",
    "        \n",
    "        termval = self.factor()\n",
    "        while self._accept('TIMES') or self._accept('DIVIDE'):\n",
    "            op = self.tok.type\n",
    "            right = self.factor()\n",
    "            if op == 'TIMES':\n",
    "                termval *= right\n",
    "            elif op == 'DIVIDE':\n",
    "                termval /= right\n",
    "        return termval\n",
    "    \n",
    "    def factor(self):\n",
    "        \"factor ::= NUM | ( expr )\"\n",
    "        \n",
    "        if self._accept('NUM'):\n",
    "            return int(self.tok.value)\n",
    "        elif self._accept('LPAREN'):\n",
    "            exprval = self.expr()\n",
    "            self._expect('RPAREN')\n",
    "            return exprval\n",
    "        else:\n",
    "            raise SyntaxError('Expected NUMBER or LPAREN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is an example of using the `ExpressionEvaluator` class interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ExpressionEvaluator at 0x104f86c88>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = ExpressionEvaluator()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.parse('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.parse('2 + 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.parse('2 + 3 * 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.parse('2 + (3 + 4) * 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we put an asterisk right after the plus operator:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e.parse('2 + (3 + * 4)')\n",
    "\n",
    "Traceback (most recent call last):\n",
    "\n",
    "  File \"/Users/benjamingrove/.pyenv/versions/3.6.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
    "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "\n",
    "  File \"<ipython-input-419-c3340b8a9b46>\", line 1, in <module>\n",
    "    e.parse('2 + (3 + * 4)')\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 42, in parse\n",
    "    return self.expr()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 69, in expr\n",
    "    right = self.term()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 79, in term\n",
    "    termval = self.factor()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 95, in factor\n",
    "    exprval = self.expr()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 69, in expr\n",
    "    right = self.term()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 79, in term\n",
    "    termval = self.factor()\n",
    "\n",
    "  File \"<ipython-input-413-948b1db97d25>\", line 99, in factor\n",
    "    raise SyntaxError('Expected NUMBER or LPAREN')\n",
    "\n",
    "  File \"<string>\", line unknown\n",
    "SyntaxError: Expected NUMBER or LPAREN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do something other than pure evaluation, you need to change the `ExpressionEvaluator` class to do something else.  \n",
    "For example, here is an alternative implementation that constructs a simple parse tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
